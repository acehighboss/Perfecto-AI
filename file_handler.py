import os
import tempfile
import asyncio
import streamlit as st
from langchain_core.documents import Document as LangChainDocument
from langchain_community.document_loaders import WebBaseLoader
from llama_parse import LlamaParse

class FileHandler:
    def __init__(self):
        self.llama_api_key = st.secrets["LLAMA_CLOUD_API_KEY"]
    
    def read_txt_file(self, file):
        """TXT íŒŒì¼ ì½ê¸°"""
        try:
            # íŒŒì¼ ë‚´ìš©ì„ ë°”ì´íŠ¸ë¡œ ì½ê³  UTF-8ë¡œ ë””ì½”ë”©
            content = file.read()
            if isinstance(content, bytes):
                # ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
                encodings = ['utf-8', 'cp949', 'euc-kr', 'latin-1']
                for encoding in encodings:
                    try:
                        text = content.decode(encoding)
                        return text
                    except UnicodeDecodeError:
                        continue
                # ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ì²˜ë¦¬
                text = content.decode('utf-8', errors='ignore')
                st.warning("ì¼ë¶€ ë¬¸ìê°€ ì˜¬ë°”ë¥´ê²Œ ì½íˆì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                return text
            else:
                return content
        except Exception as e:
            st.error(f"TXT íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            return ""

    async def parse_with_llamaparse(self, uploaded_files):
        """LlamaParseë¥¼ ì‚¬ìš©í•œ íŒŒì¼ íŒŒì‹± (PDF, DOCXë§Œ)"""
        parser = LlamaParse(
            api_key=self.llama_api_key,
            result_type="markdown",
            verbose=True,
        )
        
        parsed_data = []
        for file in uploaded_files:
            # TXT íŒŒì¼ì€ LlamaParse ì‚¬ìš©í•˜ì§€ ì•Šê³  ì§ì ‘ ì²˜ë¦¬
            if file.name.endswith('.txt'):
                text = self.read_txt_file(file)
                if text:
                    # ì„ì‹œ Document ê°ì²´ ìƒì„±
                    class TempDoc:
                        def __init__(self, text, metadata):
                            self.text = text
                            self.metadata = metadata
                    
                    parsed_data.append(TempDoc(text, {"source": file.name, "type": "txt"}))
                continue
            
            # PDF, DOCX íŒŒì¼ì€ LlamaParse ì‚¬ìš©
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.name)[1]) as tmp_file:
                tmp_file.write(file.getvalue())
                tmp_file_path = tmp_file.name
            
            try:
                documents = await parser.aload_data(tmp_file_path)
                for doc in documents:
                    # ë©”íƒ€ë°ì´í„°ì— íŒŒì¼ íƒ€ì… ì¶”ê°€
                    if hasattr(doc, 'metadata'):
                        doc.metadata["type"] = "llamaparse"
                    else:
                        doc.metadata = {"source": file.name, "type": "llamaparse"}
                parsed_data.extend(documents)
            except Exception as e:
                st.error(f"LlamaParse ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({file.name}): {e}")
            finally:
                os.remove(tmp_file_path)
        
        return parsed_data

    def get_documents_from_files(self, uploaded_files):
        """íŒŒì¼ì—ì„œ ë¬¸ì„œ ì¶”ì¶œ - TXT íŒŒì¼ ì§€ì›"""
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        llama_index_documents = loop.run_until_complete(
            self.parse_with_llamaparse(uploaded_files)
        )
        
        if llama_index_documents:
            langchain_documents = []
            for doc in llama_index_documents:
                # ë¬¸ì„œ íƒ€ì…ë³„ ë©”íƒ€ë°ì´í„° ì„¤ì •
                metadata = doc.metadata if hasattr(doc, 'metadata') else {"source": "unknown"}
                
                langchain_doc = LangChainDocument(
                    page_content=doc.text, 
                    metadata=metadata
                )
                langchain_documents.append(langchain_doc)
            
            # ì²˜ë¦¬ëœ íŒŒì¼ ì •ë³´ í‘œì‹œ
            txt_files = [doc for doc in llama_index_documents if doc.metadata.get("type") == "txt"]
            llamaparse_files = [doc for doc in llama_index_documents if doc.metadata.get("type") == "llamaparse"]
            
            if txt_files:
                st.info(f"ğŸ“„ TXT íŒŒì¼ {len(txt_files)}ê°œ ì§ì ‘ ì²˜ë¦¬ ì™„ë£Œ")
            if llamaparse_files:
                st.info(f"ğŸ” PDF/DOCX íŒŒì¼ {len(llamaparse_files)}ê°œ LlamaParse ì²˜ë¦¬ ì™„ë£Œ")
            
            return langchain_documents
        return []

    def get_documents_from_url(self, url):
        """URLì—ì„œ ë¬¸ì„œ ì¶”ì¶œ"""
        try:
            loader = WebBaseLoader(url)
            documents = loader.load()
            
            if not documents:
                st.warning("URLì—ì„œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return []
            
            st.success(f"URLì—ì„œ {len(documents)}ê°œ ë¬¸ì„œë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return documents
            
        except Exception as e:
            st.error(f"URL ë¡œë”© ì˜¤ë¥˜: {e}")
            return []
