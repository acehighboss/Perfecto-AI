import streamlit as st
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.vectorstores import FAISS
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

class RAGPipeline:
    def __init__(self):
        self.google_api_key = st.secrets["GOOGLE_API_KEY"]
        
        # Google ë¬´ë£Œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (text-embedding-004)
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=self.google_api_key
        )
        
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash", 
            temperature=0,
            google_api_key=self.google_api_key
        )

    def get_system_prompt_template(self, system_prompt):
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
        template = f"""{system_prompt}

Answer the user's question based on the context provided below and the conversation history.
The context may include text and tables in markdown format. You must be able to understand and answer based on them.
If you don't know the answer, just say that you don't know. Don't make up an answer.

Context:
{{context}}
"""
        return template

    def create_retriever(self, documents):
        """ë¬¸ì„œì—ì„œ ê²€ìƒ‰ê¸° ìƒì„±"""
        if not documents:
            st.warning("ë¬¸ì„œì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        # SemanticChunker ì‚¬ìš© (Google ì„ë² ë”© ëª¨ë¸ê³¼ í•¨ê»˜)
        text_splitter = SemanticChunker(
            self.embeddings, 
            breakpoint_threshold_type="percentile"
        )
        
        splits = text_splitter.split_documents(documents)
        
        if not splits:
            st.warning("ë¬¸ì„œ ë¶„í• ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
        
        st.info(f"ğŸ“Š ë¶„í•  ì™„ë£Œ: {len(splits)}ê°œ ì²­í¬ ìƒì„± (Google text-embedding-004 ëª¨ë¸ ì‚¬ìš©)")
        
        # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        try:
            vectorstore = FAISS.from_documents(splits, self.embeddings)
        except Exception as e:
            st.error(f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì˜¤ë¥˜: {e}")
            return None

        # ê²€ìƒ‰ê¸° ì„¤ì •
        base_retriever = vectorstore.as_retriever(
            search_type="similarity", 
            search_kwargs={"k": 10}
        )
        
        # ì••ì¶• ê²€ìƒ‰ê¸° ì‚¬ìš©
        compressor = LLMChainExtractor.from_llm(self.llm)
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=base_retriever
        )
        
        return compression_retriever

    def create_conversational_rag_chain(self, retriever, system_prompt):
        """ëŒ€í™”í˜• RAG ì²´ì¸ ìƒì„±"""
        enhanced_template = self.get_system_prompt_template(system_prompt)
        
        rag_prompt = ChatPromptTemplate.from_messages([
            ("system", enhanced_template),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{input}"),
        ])
        
        document_chain = create_stuff_documents_chain(self.llm, rag_prompt)
        return create_retrieval_chain(retriever, document_chain)

    def create_default_chain(self, system_prompt):
        """ê¸°ë³¸ ì²´ì¸ ìƒì„±"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{question}"),
        ])
        return prompt | self.llm | StrOutputParser()

    def format_chat_history(self, messages):
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…"""
        chat_history = []
        for msg in messages[:-1]:
            if msg["role"] == "user":
                chat_history.append(HumanMessage(content=msg["content"]))
            else:
                chat_history.append(AIMessage(content=msg["content"]))
        return chat_history
