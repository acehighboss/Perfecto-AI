import streamlit as st
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
import re

class SmartTextSplitter:
    """ë¬¸ë‹¨/ë¬¸ì¥ ê¸°ë°˜ ì§€ëŠ¥í˜• í…ìŠ¤íŠ¸ ë¶„í• ê¸°"""
    
    def __init__(self, max_tokens=3000, overlap_sentences=2):
        self.max_tokens = max_tokens
        self.overlap_sentences = overlap_sentences
    
    def estimate_tokens(self, text):
        """í† í° ìˆ˜ ì¶”ì • (í•œêµ­ì–´ ê¸°ì¤€ ëŒ€ëµ 2.5ì = 1í† í°)"""
        return len(text) // 2.5
    
    def split_by_sentences(self, text):
        """ë¬¸ì¥ë³„ ë¶„í• """
        # í•œêµ­ì–´ì™€ ì˜ì–´ ë¬¸ì¥ ë íŒ¨í„´
        sentence_pattern = r'[.!?;]\s+|[ã€‚ï¼ï¼Ÿï¼›]\s*|[\n]+'
        sentences = re.split(sentence_pattern, text)
        sentences = [s.strip() for s in sentences if s.strip()]
        return sentences
    
    def split_by_paragraphs(self, text):
        """ë¬¸ë‹¨ë³„ ë¶„í• """
        # ë¹ˆ ì¤„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ ë¶„í• 
        paragraphs = re.split(r'\n\s*\n', text)
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        return paragraphs
    
    def smart_split_documents(self, documents):
        """ì§€ëŠ¥í˜• ë¬¸ì„œ ë¶„í• """
        split_docs = []
        
        for doc in documents:
            text = doc.page_content
            source = doc.metadata.get('source', 'unknown')
            
            # 1ë‹¨ê³„: ë¬¸ë‹¨ë³„ ë¶„í•  ì‹œë„
            paragraphs = self.split_by_paragraphs(text)
            
            for para_idx, paragraph in enumerate(paragraphs):
                tokens = self.estimate_tokens(paragraph)
                
                # ë¬¸ë‹¨ì´ í† í° ì œí•œ ì´ë‚´ë¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                if tokens <= self.max_tokens:
                    split_docs.append(Document(
                        page_content=paragraph,
                        metadata={
                            'source': source,
                            'chunk_type': 'paragraph',
                            'chunk_index': para_idx,
                            'token_count': tokens
                        }
                    ))
                else:
                    # ë¬¸ë‹¨ì´ ë„ˆë¬´ ê¸¸ë©´ ë¬¸ì¥ë³„ë¡œ ë¶„í• 
                    sentences = self.split_by_sentences(paragraph)
                    current_chunk = ""
                    current_tokens = 0
                    sentence_buffer = []
                    
                    for sentence in sentences:
                        sentence_tokens = self.estimate_tokens(sentence)
                        
                        # ë¬¸ì¥ì´ í† í° ì œí•œì„ ì´ˆê³¼í•˜ë©´ ê°•ì œë¡œ ìë¦„
                        if sentence_tokens > self.max_tokens:
                            if current_chunk:
                                split_docs.append(Document(
                                    page_content=current_chunk,
                                    metadata={
                                        'source': source,
                                        'chunk_type': 'sentence_group',
                                        'chunk_index': f"{para_idx}_{len(split_docs)}",
                                        'token_count': current_tokens
                                    }
                                ))
                                current_chunk = ""
                                current_tokens = 0
                            
                            # ê¸´ ë¬¸ì¥ì„ ê°•ì œë¡œ ìë¦„
                            truncated = sentence[:int(self.max_tokens * 2.5)]
                            split_docs.append(Document(
                                page_content=truncated,
                                metadata={
                                    'source': source,
                                    'chunk_type': 'truncated_sentence',
                                    'chunk_index': f"{para_idx}_{len(split_docs)}",
                                    'token_count': self.max_tokens
                                }
                            ))
                            continue
                        
                        # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
                        if current_tokens + sentence_tokens <= self.max_tokens:
                            current_chunk += " " + sentence if current_chunk else sentence
                            current_tokens += sentence_tokens
                            sentence_buffer.append(sentence)
                        else:
                            # í˜„ì¬ ì²­í¬ ì €ì¥
                            if current_chunk:
                                split_docs.append(Document(
                                    page_content=current_chunk,
                                    metadata={
                                        'source': source,
                                        'chunk_type': 'sentence_group',
                                        'chunk_index': f"{para_idx}_{len(split_docs)}",
                                        'token_count': current_tokens
                                    }
                                ))
                            
                            # ìƒˆ ì²­í¬ ì‹œì‘ (ì˜¤ë²„ë© ì ìš©)
                            overlap_sentences = sentence_buffer[-self.overlap_sentences:] if len(sentence_buffer) > self.overlap_sentences else sentence_buffer
                            overlap_text = " ".join(overlap_sentences)
                            
                            current_chunk = overlap_text + " " + sentence if overlap_text else sentence
                            current_tokens = self.estimate_tokens(current_chunk)
                            sentence_buffer = overlap_sentences + [sentence]
                    
                    # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
                    if current_chunk:
                        split_docs.append(Document(
                            page_content=current_chunk,
                            metadata={
                                'source': source,
                                'chunk_type': 'sentence_group',
                                'chunk_index': f"{para_idx}_{len(split_docs)}",
                                'token_count': current_tokens
                            }
                        ))
        
        return split_docs

class RAGPipeline:
    def __init__(self):
        self.google_api_key = st.secrets["GOOGLE_API_KEY"]
        
        # Google ë¬´ë£Œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=self.google_api_key
        )
        
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash", 
            temperature=0,
            google_api_key=self.google_api_key
        )
        
        # ì§€ëŠ¥í˜• í…ìŠ¤íŠ¸ ë¶„í• ê¸°
        self.text_splitter = SmartTextSplitter(max_tokens=3000, overlap_sentences=2)

    def get_system_prompt_template(self, system_prompt):
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
        template = system_prompt + """

ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

**ë‹µë³€ ì§€ì¹¨:**
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ì»¨í…ìŠ¤íŠ¸ì— ê´€ë ¨ ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš°, ì¼ë°˜ì ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë³´ì¶© ì„¤ëª…í•˜ë˜ "(ì¼ë°˜ ì§€ì‹ ê¸°ë°˜)"ì´ë¼ê³  í‘œì‹œí•˜ì„¸ìš”
3. ë‹µë³€í•  ë•ŒëŠ” ì°¸ì¡°í•œ ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”
4. ì •í™•í•œ ì •ë³´ë§Œì„ ì œê³µí•˜ê³ , ì¶”ì¸¡ì´ë‚˜ ê°€ì •ì€ í”¼í•˜ì„¸ìš”
5. ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”

ì»¨í…ìŠ¤íŠ¸:
{context}
"""
        return template

    def create_retriever(self, documents):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ìƒì„± (ë¬¸ë‹¨/ë¬¸ì¥ ê¸°ë°˜)"""
        if not documents:
            st.warning("ë¬¸ì„œì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        # ì§€ëŠ¥í˜• ë¬¸ì„œ ë¶„í• 
        splits = self.text_splitter.smart_split_documents(documents)
        
        if not splits:
            st.warning("ë¬¸ì„œ ë¶„í• ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
        
        # ë¶„í•  ê²°ê³¼ í†µê³„
        paragraph_chunks = [s for s in splits if s.metadata.get('chunk_type') == 'paragraph']
        sentence_chunks = [s for s in splits if s.metadata.get('chunk_type') == 'sentence_group']
        
        st.info(f"ğŸ“Š ì§€ëŠ¥í˜• ë¶„í•  ì™„ë£Œ: {len(splits)}ê°œ ì²­í¬ (ë¬¸ë‹¨: {len(paragraph_chunks)}ê°œ, ë¬¸ì¥ê·¸ë£¹: {len(sentence_chunks)}ê°œ)")
        
        try:
            # ë²¡í„° ê²€ìƒ‰ê¸° ìƒì„±
            vectorstore = FAISS.from_documents(splits, self.embeddings)
            vector_retriever = vectorstore.as_retriever(
                search_type="similarity", 
                search_kwargs={
                    "k": 12,  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
                    "score_threshold": 0.1  # ë‚®ì€ ì„ê³„ê°’
                }
            )
            
            # BM25 ê²€ìƒ‰ê¸° ìƒì„±
            bm25_retriever = BM25Retriever.from_documents(splits)
            bm25_retriever.k = 8
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸°
            ensemble_retriever = EnsembleRetriever(
                retrievers=[bm25_retriever, vector_retriever],
                weights=[0.4, 0.6]
            )
            
            st.success("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° (ë¬¸ë‹¨/ë¬¸ì¥ ê¸°ë°˜) ìƒì„± ì™„ë£Œ")
            return ensemble_retriever
                
        except Exception as e:
            st.error(f"ê²€ìƒ‰ê¸° ìƒì„± ì˜¤ë¥˜: {e}")
            return None

    def create_conversational_rag_chain(self, retriever, system_prompt):
        """ëŒ€í™”í˜• RAG ì²´ì¸ ìƒì„±"""
        enhanced_template = self.get_system_prompt_template(system_prompt)
        
        rag_prompt = ChatPromptTemplate.from_messages([
            ("system", enhanced_template),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{input}"),
        ])
        
        document_chain = create_stuff_documents_chain(self.llm, rag_prompt)
        return create_retrieval_chain(retriever, document_chain)

    def create_default_chain(self, system_prompt):
        """ê¸°ë³¸ ì²´ì¸ ìƒì„±"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{question}"),
        ])
        return prompt | self.llm | StrOutputParser()

    def format_chat_history(self, messages):
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…"""
        chat_history = []
        for msg in messages[:-1]:
            if msg["role"] == "user":
                chat_history.append(HumanMessage(content=msg["content"]))
            else:
                chat_history.append(AIMessage(content=msg["content"]))
        return chat_history
