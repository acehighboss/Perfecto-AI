import streamlit as st
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_upstage import UpstageEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_core.documents import Document
import re
import nltk
from nltk.tokenize import sent_tokenize

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì´ˆê¸° ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

class SmartTextSplitter:
    """ë¬¸ë‹¨/ë¬¸ì¥ ê¸°ë°˜ ì§€ëŠ¥í˜• í…ìŠ¤íŠ¸ ë¶„í• ê¸°"""
    
    def __init__(self, max_chunk_size=2000, overlap_sentences=2):
        self.max_chunk_size = max_chunk_size
        self.overlap_sentences = overlap_sentences
    
    def detect_table_boundaries(self, text):
        """í‘œ ê²½ê³„ ê°ì§€"""
        table_patterns = [
            r'\|.*\|',  # ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”
            r'â”€+',      # í‘œ êµ¬ë¶„ì„ 
            r'â”Œ.*â”',    # ë°•ìŠ¤ í…Œì´ë¸”
            r'^\s*\d+\.\s*\d+',  # ìˆ«ì ë°ì´í„° íŒ¨í„´
            r'^\s*[ê°€-í£]+\s*\|\s*[ê°€-í£]+',  # í•œê¸€ í…Œì´ë¸” íŒ¨í„´
        ]
        
        table_regions = []
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            for pattern in table_patterns:
                if re.search(pattern, line):
                    # í‘œ ì‹œì‘/ë ì°¾ê¸°
                    start = max(0, i - 2)
                    end = min(len(lines), i + 10)
                    table_regions.append((start, end))
                    break
        
        return table_regions
    
    def split_by_paragraphs(self, text):
        """ë¬¸ë‹¨ë³„ ë¶„í• """
        # í‘œ ì˜ì—­ ê°ì§€
        table_regions = self.detect_table_boundaries(text)
        
        # ë¬¸ë‹¨ ë¶„í•  (ë¹ˆ ì¤„ ê¸°ì¤€)
        paragraphs = re.split(r'\n\s*\n', text)
        
        chunks = []
        current_chunk = ""
        current_size = 0
        
        for i, paragraph in enumerate(paragraphs):
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # í‘œê°€ í¬í•¨ëœ ë¬¸ë‹¨ì€ ë¶„í• í•˜ì§€ ì•ŠìŒ
            is_table_paragraph = self._is_table_content(paragraph)
            
            if is_table_paragraph:
                # í˜„ì¬ ì²­í¬ê°€ ìˆìœ¼ë©´ ì €ì¥
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                    current_size = 0
                
                # í‘œ ë¬¸ë‹¨ì€ ë…ë¦½ì ìœ¼ë¡œ ì €ì¥
                chunks.append(paragraph)
                continue
            
            # ì¼ë°˜ ë¬¸ë‹¨ ì²˜ë¦¬
            paragraph_size = len(paragraph)
            
            if current_size + paragraph_size > self.max_chunk_size and current_chunk:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                chunks.append(current_chunk.strip())
                current_chunk = paragraph
                current_size = paragraph_size
            else:
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
                current_size += paragraph_size
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def split_by_sentences(self, text):
        """ë¬¸ì¥ë³„ ë¶„í•  (ë¬¸ë‹¨ ë¶„í• ì´ ì–´ë ¤ìš´ ê²½ìš°)"""
        # í‘œ ì˜ì—­ ê°ì§€
        table_regions = self.detect_table_boundaries(text)
        
        # ë¬¸ì¥ ë¶„í• 
        sentences = sent_tokenize(text)
        
        chunks = []
        current_chunk = ""
        current_size = 0
        sentence_buffer = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # í‘œ ê´€ë ¨ ë¬¸ì¥ ê°ì§€
            is_table_sentence = self._is_table_content(sentence)
            
            if is_table_sentence:
                # í˜„ì¬ ì²­í¬ê°€ ìˆìœ¼ë©´ ì €ì¥
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                    current_size = 0
                    sentence_buffer = []
                
                # í‘œ ê´€ë ¨ ë¬¸ì¥ë“¤ì„ ëª¨ì•„ì„œ ì²˜ë¦¬
                sentence_buffer.append(sentence)
                continue
            
            # í‘œ ë¬¸ì¥ ë²„í¼ê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
            if sentence_buffer:
                table_chunk = " ".join(sentence_buffer)
                chunks.append(table_chunk)
                sentence_buffer = []
            
            # ì¼ë°˜ ë¬¸ì¥ ì²˜ë¦¬
            sentence_size = len(sentence)
            
            if current_size + sentence_size > self.max_chunk_size and current_chunk:
                # ì˜¤ë²„ë©ì„ ìœ„í•´ ë§ˆì§€ë§‰ ëª‡ ë¬¸ì¥ ìœ ì§€
                overlap_text = self._get_overlap_text(current_chunk)
                chunks.append(current_chunk.strip())
                current_chunk = overlap_text + " " + sentence if overlap_text else sentence
                current_size = len(current_chunk)
            else:
                if current_chunk:
                    current_chunk += " " + sentence
                else:
                    current_chunk = sentence
                current_size += sentence_size
        
        # ë§ˆì§€ë§‰ ì²˜ë¦¬
        if sentence_buffer:
            table_chunk = " ".join(sentence_buffer)
            chunks.append(table_chunk)
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _is_table_content(self, text):
        """í…ìŠ¤íŠ¸ê°€ í‘œ ë‚´ìš©ì¸ì§€ íŒë‹¨"""
        table_indicators = [
            r'\|.*\|',  # ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”
            r'â”€+',      # í‘œ êµ¬ë¶„ì„ 
            r'â”Œ.*â”',    # ë°•ìŠ¤ í…Œì´ë¸”
            r'^\s*\d+\s*\|\s*\d+',  # ìˆ«ì í…Œì´ë¸”
            r'^\s*[ê°€-í£]+\s*\|\s*[ê°€-í£]+',  # í•œê¸€ í…Œì´ë¸”
            r'^\s*\d+\.\d+\s*\d+\.\d+',  # ìˆ«ì ë°ì´í„° íŒ¨í„´
            r'(ë§¤ì¶œ|ìˆ˜ìµ|ë¹„ìš©|ìì‚°|ë¶€ì±„|ìë³¸).*\d+',  # ì¬ë¬´ ë°ì´í„°
            r'\d+\s*(ì–µ|ë§Œ|ì²œ|ì›|ë‹¬ëŸ¬|\$|%)',  # ë‹¨ìœ„ê°€ ìˆëŠ” ìˆ«ì
            r'(ë¶„ê¸°|ì—°ë„|ì›”ë³„|ì¼ë³„).*\d+',  # ì‹œê³„ì—´ ë°ì´í„°
        ]
        
        for pattern in table_indicators:
            if re.search(pattern, text):
                return True
        
        # ìˆ«ì ë°€ë„ ì²´í¬ (ìˆ«ìê°€ ë§ìœ¼ë©´ í‘œì¼ ê°€ëŠ¥ì„± ë†’ìŒ)
        numbers = re.findall(r'\d+', text)
        if len(numbers) > 3 and len(text) < 200:
            return True
        
        return False
    
    def _get_overlap_text(self, text):
        """ì˜¤ë²„ë©ì„ ìœ„í•œ ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤ ì¶”ì¶œ"""
        sentences = sent_tokenize(text)
        if len(sentences) <= self.overlap_sentences:
            return ""
        
        overlap_sentences = sentences[-self.overlap_sentences:]
        return " ".join(overlap_sentences)
    
    def split_documents(self, documents):
        """ë¬¸ì„œ ë¶„í•  ë©”ì¸ í•¨ìˆ˜"""
        split_docs = []
        
        for doc in documents:
            text = doc.page_content
            
            # 1ì°¨: ë¬¸ë‹¨ë³„ ë¶„í•  ì‹œë„
            try:
                chunks = self.split_by_paragraphs(text)
                
                # ë¬¸ë‹¨ë³„ ë¶„í• ì´ íš¨ê³¼ì ì´ì§€ ì•Šìœ¼ë©´ ë¬¸ì¥ë³„ ë¶„í• 
                if len(chunks) < 2 or any(len(chunk) > self.max_chunk_size * 1.5 for chunk in chunks):
                    chunks = self.split_by_sentences(text)
                
            except Exception:
                # ì‹¤íŒ¨ ì‹œ ë¬¸ì¥ë³„ ë¶„í• 
                chunks = self.split_by_sentences(text)
            
            # Document ê°ì²´ ìƒì„±
            for i, chunk in enumerate(chunks):
                if chunk.strip():
                    split_docs.append(Document(
                        page_content=chunk,
                        metadata={
                            **doc.metadata,
                            'chunk_id': i,
                            'chunk_type': 'table' if self._is_table_content(chunk) else 'text'
                        }
                    ))
        
        return split_docs

class RAGPipeline:
    def __init__(self):
        self.upstage_api_key = st.secrets["UPSTAGE_API_KEY"]
        self.google_api_key = st.secrets["GOOGLE_API_KEY"]
        self.embeddings = UpstageEmbeddings(
            api_key=self.upstage_api_key,
            model="solar-embedding-1-large"
        )
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash", 
            temperature=0,
            google_api_key=self.google_api_key
        )
        self.text_splitter = SmartTextSplitter(max_chunk_size=1500, overlap_sentences=2)

    def get_universal_table_prompt(self, system_prompt):
        """ë²”ìš© í‘œ í•´ì„ í”„ë¡¬í”„íŠ¸"""
        return f"""{system_prompt}

ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ëŠ” ë¬¸ë‹¨ë³„/ë¬¸ì¥ë³„ë¡œ ì˜ë¯¸ì ìœ¼ë¡œ ë¶„í• ë˜ì–´ ìˆìœ¼ë©°, í‘œ êµ¬ì¡°ê°€ ë³´ì¡´ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**ì§€ëŠ¥í˜• ë¶„í•  ì‹œìŠ¤í…œ íŠ¹ì§•:**
- í‘œ ë‚´ìš©ì€ ë¶„í• ë˜ì§€ ì•Šê³  ì™„ì „í•œ í˜•íƒœë¡œ ì œê³µë©ë‹ˆë‹¤
- ë¬¸ë‹¨ë³„ ë¶„í• ë¡œ ì˜ë¯¸ì  ì—°ê´€ì„±ì´ ë†’ìŠµë‹ˆë‹¤
- ë¬¸ì¥ë³„ ì˜¤ë²„ë©ìœ¼ë¡œ ë§¥ë½ì´ ë³´ì¡´ë©ë‹ˆë‹¤

**ë²”ìš© í‘œ í•´ì„ ì§€ì¹¨:**

1. **í‘œ êµ¬ì¡° ë¶„ì„:**
   - ì™„ì „í•œ í‘œ êµ¬ì¡°ë¥¼ í™œìš©í•˜ì—¬ ì •í™•í•œ í•´ì„
   - í—¤ë”ì™€ ë°ì´í„°ì˜ ê´€ê³„ë¥¼ ëª…í™•íˆ íŒŒì•…
   - ë³‘í•©ëœ ì…€ì´ë‚˜ ë‹¤ë‹¨ê³„ í—¤ë” ê³ ë ¤
   - í‘œ ì œëª©ê³¼ ìº¡ì…˜ì˜ ë§¥ë½ í™œìš©

2. **ë°ì´í„° ì •í™•ì„±:**
   - ìˆ«ì ë°ì´í„°ì™€ ë‹¨ìœ„ë¥¼ ì •í™•íˆ ë§¤ì¹­
   - ë‚ ì§œ/ì‹œê°„ í˜•ì‹ì„ ì •í™•íˆ í•´ì„
   - ë°±ë¶„ìœ¨ê³¼ ë¹„ìœ¨ ì •ë³´ ì •í™•íˆ ì²˜ë¦¬
   - í†µí™” ë‹¨ìœ„ ëª…ì‹œ

3. **ì˜ë¯¸ì  ì—°ê´€ì„±:**
   - ë¬¸ë‹¨ë³„ ë¶„í• ë¡œ ê´€ë ¨ ì •ë³´ê°€ í•¨ê»˜ ì œê³µë¨
   - í‘œì™€ ê´€ë ¨ëœ ì„¤ëª… í…ìŠ¤íŠ¸ ì—°ê²°
   - ì‹œê³„ì—´ ë°ì´í„°ì˜ ë³€í™” ì¶”ì´ ë¶„ì„
   - ì¹´í…Œê³ ë¦¬ë³„ ë¹„êµ ë¶„ì„

4. **ë‹µë³€ í˜•ì‹:**
   - ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€
   - í‘œ í˜•íƒœ ì •ë¦¬ ì‹œ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ì‚¬ìš©
   - ë‹¨ìœ„ì™€ í•¨ê»˜ ì •í™•í•œ ìˆ˜ì¹˜ ì œê³µ
   - ì¶œì²˜ ëª…ì‹œ í•„ìˆ˜

**íŠ¹ë³„ ì§€ì¹¨:**
- í‘œ ì²­í¬ëŠ” ì™„ì „í•œ í˜•íƒœë¡œ ì œê³µë˜ë¯€ë¡œ ì •í™•í•œ í•´ì„ ê°€ëŠ¥
- ë¬¸ë‹¨ë³„ ë§¥ë½ì„ í™œìš©í•˜ì—¬ í¬ê´„ì  ë‹µë³€ ì œê³µ
- í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” "í™•ì‹¤í•˜ì§€ ì•ŠìŒ"ìœ¼ë¡œ ëª…ì‹œ
- ê³„ì‚°ì´ í•„ìš”í•œ ê²½ìš° ê³¼ì •ì„ ì„¤ëª…

ì»¨í…ìŠ¤íŠ¸:
{context}
"""

    def create_retriever(self, documents):
        """ë¬¸ì„œì—ì„œ ê²€ìƒ‰ê¸° ìƒì„± - ì§€ëŠ¥í˜• ë¶„í•  ì ìš©"""
        if not documents:
            st.warning("ë¬¸ì„œì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        # ì§€ëŠ¥í˜• í…ìŠ¤íŠ¸ ë¶„í•  ì ìš©
        splits = self.text_splitter.split_documents(documents)
        
        if not splits:
            st.warning("ë¬¸ì„œ ë¶„í• ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
        
        # ë¶„í•  ê²°ê³¼ ì •ë³´ í‘œì‹œ
        table_chunks = [doc for doc in splits if doc.metadata.get('chunk_type') == 'table']
        text_chunks = [doc for doc in splits if doc.metadata.get('chunk_type') == 'text']
        
        st.info(f"ğŸ“Š ì§€ëŠ¥í˜• ë¶„í•  ì™„ë£Œ: í‘œ ì²­í¬ {len(table_chunks)}ê°œ, í…ìŠ¤íŠ¸ ì²­í¬ {len(text_chunks)}ê°œ")
        
        # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        try:
            vectorstore = FAISS.from_documents(splits, self.embeddings)
        except Exception as e:
            st.error(f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì˜¤ë¥˜: {e}")
            return None

        # ê²€ìƒ‰ê¸° ì„¤ì • - í‘œ ë°ì´í„°ë¥¼ ìœ„í•´ ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
        base_retriever = vectorstore.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                "k": 12,  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰ (í‘œ + ê´€ë ¨ í…ìŠ¤íŠ¸)
                "score_threshold": 0.15  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ í¬í•¨
            }
        )
        
        # ì••ì¶• ê²€ìƒ‰ê¸° ì‚¬ìš©
        compressor = LLMChainExtractor.from_llm(self.llm)
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=base_retriever
        )
        
        return compression_retriever

    def create_conversational_rag_chain(self, retriever, system_prompt):
        """ëŒ€í™”í˜• RAG ì²´ì¸ ìƒì„±"""
        enhanced_template = self.get_universal_table_prompt(system_prompt)
        
        rag_prompt = ChatPromptTemplate.from_messages([
            ("system", enhanced_template),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{input}"),
        ])
        
        document_chain = create_stuff_documents_chain(self.llm, rag_prompt)
        return create_retrieval_chain(retriever, document_chain)

    def create_default_chain(self, system_prompt):
        """ê¸°ë³¸ ì²´ì¸ ìƒì„±"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{question}"),
        ])
        return prompt | self.llm | StrOutputParser()

    def format_chat_history(self, messages):
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…"""
        chat_history = []
        for msg in messages[:-1]:
            if msg["role"] == "user":
                chat_history.append(HumanMessage(content=msg["content"]))
            else:
                chat_history.append(AIMessage(content=msg["content"]))
        return chat_history

    def validate_table_response(self, user_input, ai_answer, source_documents):
        """í‘œ ê´€ë ¨ ë‹µë³€ ê²€ì¦"""
        table_indicators = [
            "í‘œ", "í…Œì´ë¸”", "table", "ë§¤ì¶œ", "ì‹¤ì ", "ë°ì´í„°", "ìˆ˜ì¹˜", "ê¸ˆì•¡", 
            "ë¹„ìœ¨", "í¼ì„¼íŠ¸", "%", "ë¶„ê¸°", "ì—°ë„", "ë¶€ë¬¸", "í•­ëª©", "í•©ê³„", 
            "í‰ê· ", "ìµœëŒ€", "ìµœì†Œ", "ì¦ê°€", "ê°ì†Œ", "ë¹„êµ", "ìˆœìœ„"
        ]
        
        is_table_question = any(indicator in user_input.lower() for indicator in table_indicators)
        
        if is_table_question:
            # í‘œ ì²­í¬ê°€ ìˆëŠ”ì§€ í™•ì¸
            table_chunks = [doc for doc in source_documents 
                          if doc.metadata.get('chunk_type') == 'table']
            
            if table_chunks and ("í™•ì‹¤í•˜ì§€ ì•ŠìŒ" in ai_answer or "í•´ë‹¹ ì •ë³´ ì—†ìŒ" in ai_answer):
                return {
                    "warning": "âš ï¸ í‘œ ë°ì´í„°ê°€ ì™„ì „í•œ í˜•íƒœë¡œ ì œê³µë˜ì—ˆì§€ë§Œ í•´ì„ì— ì–´ë ¤ì›€ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "suggestion": "ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ 'í‘œ í˜•íƒœë¡œ ì •ë¦¬í•´ë‹¬ë¼'ê³  ìš”ì²­í•´ë³´ì„¸ìš”."
                }
            
            if not table_chunks:
                return {
                    "warning": "âš ï¸ í‘œ ê´€ë ¨ ì§ˆë¬¸ì´ì§€ë§Œ ê´€ë ¨ í‘œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                    "suggestion": "ë¬¸ì„œì— í•´ë‹¹ í‘œê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš”."
                }
        
        return None
