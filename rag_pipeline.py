import streamlit as st
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.vectorstores import FAISS
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

class RAGPipeline:
    def __init__(self):
        self.google_api_key = st.secrets["GOOGLE_API_KEY"]
        
        # Google ë¬´ë£Œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (text-embedding-004)
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=self.google_api_key
        )
        
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash", 
            temperature=0,
            google_api_key=self.google_api_key
        )

    def get_system_prompt_template(self, system_prompt):
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ - context ë³€ìˆ˜ ìˆ˜ì •"""
        # f-string ì‚¬ìš©í•˜ì§€ ì•Šê³  ì§ì ‘ ë¬¸ìì—´ ì—°ê²°
        template = system_prompt + """

ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì—ëŠ” ë¶„ì„ëœ ë¬¸ì„œì˜ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**ì¤‘ìš”í•œ ì§€ì¹¨:**
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ ë°˜ë“œì‹œ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ì»¨í…ìŠ¤íŠ¸ì— ê´€ë ¨ ì •ë³´ê°€ ìˆë‹¤ë©´ "ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì§€ ë§ˆì„¸ìš”
3. ì»¨í…ìŠ¤íŠ¸ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
4. ë‹µë³€í•  ë•ŒëŠ” ì°¸ì¡°í•œ ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”

ì»¨í…ìŠ¤íŠ¸:
{context}
"""
        return template

    def create_retriever(self, documents):
        """ë¬¸ì„œì—ì„œ ê²€ìƒ‰ê¸° ìƒì„±"""
        if not documents:
            st.warning("ë¬¸ì„œì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        # SemanticChunker ì‚¬ìš© (Google ì„ë² ë”© ëª¨ë¸ê³¼ í•¨ê»˜)
        try:
            text_splitter = SemanticChunker(
                self.embeddings, 
                breakpoint_threshold_type="percentile"
            )
            
            splits = text_splitter.split_documents(documents)
        except Exception as e:
            st.warning(f"SemanticChunker ì‹¤íŒ¨, ê¸°ë³¸ ë¶„í•  ì‚¬ìš©: {e}")
            # ê¸°ë³¸ ë¶„í• ê¸°ë¡œ ëŒ€ì²´
            from langchain_text_splitters import RecursiveCharacterTextSplitter
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=100
            )
            splits = text_splitter.split_documents(documents)
        
        if not splits:
            st.warning("ë¬¸ì„œ ë¶„í• ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
        
        st.info(f"ğŸ“Š ë¶„í•  ì™„ë£Œ: {len(splits)}ê°œ ì²­í¬ ìƒì„±")
        
        # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        try:
            vectorstore = FAISS.from_documents(splits, self.embeddings)
        except Exception as e:
            st.error(f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì˜¤ë¥˜: {e}")
            return None

        # ê²€ìƒ‰ê¸° ì„¤ì • - ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
        base_retriever = vectorstore.as_retriever(
            search_type="similarity", 
            search_kwargs={"k": 8}  # ë” ë§ì€ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        )
        
        # ì••ì¶• ê²€ìƒ‰ê¸° ì‚¬ìš©í•˜ì§€ ì•Šê³  ê¸°ë³¸ ê²€ìƒ‰ê¸°ë§Œ ì‚¬ìš© (ì•ˆì •ì„± ìš°ì„ )
        return base_retriever

    def create_conversational_rag_chain(self, retriever, system_prompt):
        """ëŒ€í™”í˜• RAG ì²´ì¸ ìƒì„±"""
        enhanced_template = self.get_system_prompt_template(system_prompt)
        
        rag_prompt = ChatPromptTemplate.from_messages([
            ("system", enhanced_template),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{input}"),
        ])
        
        document_chain = create_stuff_documents_chain(self.llm, rag_prompt)
        return create_retrieval_chain(retriever, document_chain)

    def create_default_chain(self, system_prompt):
        """ê¸°ë³¸ ì²´ì¸ ìƒì„±"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{question}"),
        ])
        return prompt | self.llm | StrOutputParser()

    def format_chat_history(self, messages):
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…"""
        chat_history = []
        for msg in messages[:-1]:
            if msg["role"] == "user":
                chat_history.append(HumanMessage(content=msg["content"]))
            else:
                chat_history.append(AIMessage(content=msg["content"]))
        return chat_history
