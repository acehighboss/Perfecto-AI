import streamlit as st
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import FAISS
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.documents import Document
import re
import tiktoken

class IntelligentTextSplitter:
    """ë¬¸ë‹¨ë³„/ë¬¸ì¥ë³„ ì§€ëŠ¥í˜• ë¶„í•  ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_tokens=3000):
        self.max_tokens = max_tokens
        self.encoder = tiktoken.get_encoding("cl100k_base")
    
    def count_tokens(self, text):
        """í† í° ìˆ˜ ê³„ì‚°"""
        return len(self.encoder.encode(text))
    
    def split_by_sentences(self, text):
        """ë¬¸ì¥ë³„ ë¶„í• """
        sentence_patterns = [
            r'[.!?]\s+',
            r'[ã€‚ï¼ï¼Ÿ]\s*',
            r'\n\s*\n',
        ]
        
        sentences = []
        current_text = text
        
        for pattern in sentence_patterns:
            parts = re.split(pattern, current_text)
            if len(parts) > 1:
                sentences.extend([part.strip() for part in parts if part.strip()])
                break
        
        if not sentences:
            sentences = [text]
        
        return sentences
    
    def split_by_paragraphs(self, text):
        """ë¬¸ë‹¨ë³„ ë¶„í• """
        paragraphs = re.split(r'\n\s*\n', text)
        return [p.strip() for p in paragraphs if p.strip()]
    
    def smart_split(self, text):
        """ì§€ëŠ¥í˜• ë¶„í•  (ë¬¸ë‹¨ ìš°ì„ , í•„ìš”ì‹œ ë¬¸ì¥ë³„)"""
        paragraphs = self.split_by_paragraphs(text)
        chunks = []
        
        for paragraph in paragraphs:
            token_count = self.count_tokens(paragraph)
            
            if token_count <= self.max_tokens:
                chunks.append(paragraph)
            else:
                # ë¬¸ë‹¨ì´ ë„ˆë¬´ ê¸¸ë©´ ë¬¸ì¥ë³„ë¡œ ë¶„í• 
                sentences = self.split_by_sentences(paragraph)
                current_chunk = ""
                current_tokens = 0
                
                for sentence in sentences:
                    sentence_tokens = self.count_tokens(sentence)
                    
                    if sentence_tokens > self.max_tokens:
                        # ë¬¸ì¥ë„ ë„ˆë¬´ ê¸¸ë©´ ê°•ì œ ë¶„í• 
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                            current_chunk = ""
                            current_tokens = 0
                        
                        # ê¸´ ë¬¸ì¥ì„ í† í° ë‹¨ìœ„ë¡œ ë¶„í• 
                        tokens = self.encoder.encode(sentence)
                        for i in range(0, len(tokens), self.max_tokens):
                            chunk_tokens = tokens[i:i+self.max_tokens]
                            chunk_text = self.encoder.decode(chunk_tokens)
                            chunks.append(chunk_text)
                    else:
                        if current_tokens + sentence_tokens > self.max_tokens:
                            if current_chunk:
                                chunks.append(current_chunk.strip())
                            current_chunk = sentence
                            current_tokens = sentence_tokens
                        else:
                            if current_chunk:
                                current_chunk += " " + sentence
                            else:
                                current_chunk = sentence
                            current_tokens += sentence_tokens
                
                if current_chunk:
                    chunks.append(current_chunk.strip())
        
        return chunks
    
    def split_documents(self, documents):
        """ë¬¸ì„œ ë¶„í• """
        split_docs = []
        
        for doc in documents:
            chunks = self.smart_split(doc.page_content)
            
            for i, chunk in enumerate(chunks):
                if chunk.strip():
                    split_docs.append(Document(
                        page_content=chunk,
                        metadata={
                            **doc.metadata,
                            'chunk_id': i,
                            'token_count': self.count_tokens(chunk)
                        }
                    ))
        
        return split_docs

class RAGPipeline:
    def __init__(self):
        self.google_api_key = st.secrets["GOOGLE_API_KEY"]
        
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=self.google_api_key
        )
        
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash", 
            temperature=0,
            google_api_key=self.google_api_key
        )
        
        self.text_splitter = IntelligentTextSplitter(max_tokens=3000)

    def get_system_prompt_template(self, system_prompt):
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
        template = system_prompt + """

ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

**ì¤‘ìš”í•œ ì§€ì¹¨:**
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¼¼ê¼¼íˆ ê²€í† í•˜ê³  ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€í•˜ì„¸ìš”
2. ì»¨í…ìŠ¤íŠ¸ì— ê´€ë ¨ ë‚´ìš©ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
3. ë‹µë³€í•  ë•ŒëŠ” ì°¸ì¡°í•œ ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”
4. ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
5. ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶©ë¶„íˆ í™œìš©í•˜ì—¬ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•˜ì„¸ìš”

ì»¨í…ìŠ¤íŠ¸:
{context}
"""
        return template

    def create_retriever(self, documents):
        """í–¥ìƒëœ ê²€ìƒ‰ê¸° ìƒì„±"""
        if not documents:
            st.warning("ë¬¸ì„œì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        # ì§€ëŠ¥í˜• ë¶„í•  ì ìš©
        splits = self.text_splitter.split_documents(documents)
        
        if not splits:
            st.warning("ë¬¸ì„œ ë¶„í• ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
        
        # í† í° ìˆ˜ ì •ë³´ í‘œì‹œ
        total_tokens = sum(doc.metadata.get('token_count', 0) for doc in splits)
        st.info(f"ğŸ“Š ì§€ëŠ¥í˜• ë¶„í•  ì™„ë£Œ: {len(splits)}ê°œ ì²­í¬, ì´ {total_tokens:,} í† í°")
        
        # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        try:
            vectorstore = FAISS.from_documents(splits, self.embeddings)
        except Exception as e:
            st.error(f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì˜¤ë¥˜: {e}")
            return None

        # ë” ë§ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ê´€ë ¨ ë‚´ìš© ëˆ„ë½ ë°©ì§€
        retriever = vectorstore.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                "k": 5,  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
                "score_threshold": 0.5  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë” ë§ì€ í›„ë³´ í¬í•¨
            }
        )
        
        return retriever

    def create_conversational_rag_chain(self, retriever, system_prompt):
        """ëŒ€í™”í˜• RAG ì²´ì¸ ìƒì„±"""
        enhanced_template = self.get_system_prompt_template(system_prompt)
        
        rag_prompt = ChatPromptTemplate.from_messages([
            ("system", enhanced_template),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{input}"),
        ])
        
        document_chain = create_stuff_documents_chain(self.llm, rag_prompt)
        return create_retrieval_chain(retriever, document_chain)

    def create_default_chain(self, system_prompt):
        """ê¸°ë³¸ ì²´ì¸ ìƒì„±"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{question}"),
        ])
        return prompt | self.llm | StrOutputParser()

    def format_chat_history(self, messages):
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…"""
        chat_history = []
        for msg in messages[:-1]:
            if msg["role"] == "user":
                chat_history.append(HumanMessage(content=msg["content"]))
            else:
                chat_history.append(AIMessage(content=msg["content"]))
        return chat_history
