import streamlit as st
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
import re

class SmartTextSplitter:
    """ê°œì„ ëœ ë¬¸ë‹¨/ë¬¸ì¥ ê¸°ë°˜ ì§€ëŠ¥í˜• í…ìŠ¤íŠ¸ ë¶„í• ê¸°"""
    
    def __init__(self, max_chunk_size=1000, overlap_sentences=1):
        self.max_chunk_size = max_chunk_size
        self.overlap_sentences = overlap_sentences
    
    def flexible_sentence_split(self, text):
        """ìœ ì—°í•œ ë¬¸ì¥ ë¶„í•  - ë§ˆì¹¨í‘œ ì—†ëŠ” ë¬¸ì¥ë„ ê³ ë ¤"""
        # ëª…í™•í•œ ë¬¸ì¥ ë íŒ¨í„´
        sentence_endings = r'[.!?;]["\']?\s+'
        
        # 1ì°¨: ëª…í™•í•œ ë¬¸ì¥ ëìœ¼ë¡œ ë¶„í• 
        sentences = re.split(sentence_endings, text)
        
        # 2ì°¨: ë§ˆì¹¨í‘œ ì—†ëŠ” ë¬¸ì¥ë“¤ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„í• 
        refined_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # ê¸´ ë¬¸ì¥ì´ê³  ì¤„ë°”ê¿ˆì´ ìˆìœ¼ë©´ ì¤„ë°”ê¿ˆ ë‹¨ìœ„ë¡œ ì¶”ê°€ ë¶„í• 
            if len(sentence) > 200 and '\n' in sentence:
                lines = sentence.split('\n')
                for line in lines:
                    line = line.strip()
                    if line:
                        refined_sentences.append(line)
            else:
                refined_sentences.append(sentence)
        
        # 3ì°¨: ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ë“¤ì„ ì¸ì ‘í•œ ë¬¸ì¥ê³¼ ë³‘í•©
        final_sentences = []
        current_sentence = ""
        
        for sentence in refined_sentences:
            if len(sentence) < 50 and current_sentence:
                # ì§§ì€ ë¬¸ì¥ì€ ì´ì „ ë¬¸ì¥ê³¼ ë³‘í•©
                current_sentence += " " + sentence
            else:
                if current_sentence:
                    final_sentences.append(current_sentence)
                current_sentence = sentence
        
        if current_sentence:
            final_sentences.append(current_sentence)
        
        return final_sentences
    
    def split_by_paragraphs(self, text):
        """ë¬¸ë‹¨ë³„ ë¶„í•  (ìš°ì„  ë°©ë²•)"""
        # ë¬¸ë‹¨ ë¶„í•  (ë¹ˆ ì¤„ ê¸°ì¤€)
        paragraphs = re.split(r'\n\s*\n', text)
        
        chunks = []
        current_chunk = ""
        current_size = 0
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            paragraph_size = len(paragraph)
            
            # í† í° ì œí•œ í™•ì¸
            if current_size + paragraph_size > self.max_chunk_size and current_chunk:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                chunks.append(current_chunk.strip())
                current_chunk = paragraph
                current_size = paragraph_size
            else:
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
                current_size += paragraph_size
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def split_by_sentences(self, text):
        """ë¬¸ì¥ë³„ ë¶„í•  (í† í° ì œí•œ ì´ˆê³¼ ì‹œ)"""
        sentences = self.flexible_sentence_split(text)
        
        chunks = []
        current_chunk = ""
        current_size = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            sentence_size = len(sentence)
            
            # ë‹¨ì¼ ë¬¸ì¥ì´ í† í° ì œí•œì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš°
            if sentence_size > self.max_chunk_size:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                    current_size = 0
                
                # ê¸´ ë¬¸ì¥ì„ ê°•ì œë¡œ ë¶„í• 
                words = sentence.split()
                temp_chunk = ""
                for word in words:
                    if len(temp_chunk + " " + word) > self.max_chunk_size:
                        if temp_chunk:
                            chunks.append(temp_chunk.strip())
                        temp_chunk = word
                    else:
                        temp_chunk += " " + word if temp_chunk else word
                
                if temp_chunk:
                    chunks.append(temp_chunk.strip())
                continue
            
            # í† í° ì œí•œ í™•ì¸
            if current_size + sentence_size > self.max_chunk_size and current_chunk:
                # ì˜¤ë²„ë© ì²˜ë¦¬
                overlap_text = self._get_overlap_text(current_chunk)
                chunks.append(current_chunk.strip())
                current_chunk = overlap_text + " " + sentence if overlap_text else sentence
                current_size = len(current_chunk)
            else:
                if current_chunk:
                    current_chunk += " " + sentence
                else:
                    current_chunk = sentence
                current_size += sentence_size
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _get_overlap_text(self, text):
        """ì˜¤ë²„ë©ì„ ìœ„í•œ ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤ ì¶”ì¶œ"""
        sentences = self.flexible_sentence_split(text)
        if len(sentences) <= self.overlap_sentences:
            return ""
        
        overlap_sentences = sentences[-self.overlap_sentences:]
        return " ".join(overlap_sentences)
    
    def split_documents(self, documents):
        """ë¬¸ì„œ ë¶„í•  ë©”ì¸ í•¨ìˆ˜"""
        split_docs = []
        
        for doc in documents:
            text = doc.page_content
            
            # 1ì°¨: ë¬¸ë‹¨ë³„ ë¶„í•  ì‹œë„
            try:
                chunks = self.split_by_paragraphs(text)
                
                # ë¬¸ë‹¨ë³„ ë¶„í•  ê²°ê³¼ ê²€ì¦
                oversized_chunks = [chunk for chunk in chunks if len(chunk) > self.max_chunk_size * 1.2]
                
                if oversized_chunks:
                    # í† í° ì œí•œ ì´ˆê³¼ ì‹œ ë¬¸ì¥ë³„ ë¶„í• 
                    st.info(f"í† í° ì œí•œ ì´ˆê³¼ë¡œ ë¬¸ì¥ë³„ ë¶„í•  ì ìš© ({len(oversized_chunks)}ê°œ ì²­í¬)")
                    chunks = self.split_by_sentences(text)
                
            except Exception as e:
                st.warning(f"ì§€ëŠ¥í˜• ë¶„í•  ì‹¤íŒ¨, ê¸°ë³¸ ë¶„í•  ì‚¬ìš©: {e}")
                # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ RecursiveCharacterTextSplitter ì‚¬ìš©
                fallback_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=self.max_chunk_size,
                    chunk_overlap=100
                )
                fallback_docs = fallback_splitter.split_documents([doc])
                chunks = [d.page_content for d in fallback_docs]
            
            # Document ê°ì²´ ìƒì„±
            for i, chunk in enumerate(chunks):
                if chunk.strip():
                    split_docs.append(Document(
                        page_content=chunk,
                        metadata={
                            **doc.metadata,
                            'chunk_id': i,
                            'chunk_length': len(chunk)
                        }
                    ))
        
        return split_docs

class RAGPipeline:
    def __init__(self):
        self.google_api_key = st.secrets["GOOGLE_API_KEY"]
        
        # Google ë¬´ë£Œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (text-embedding-004)
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=self.google_api_key
        )
        
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash", 
            temperature=0,
            google_api_key=self.google_api_key
        )
        
        # ê°œì„ ëœ í…ìŠ¤íŠ¸ ë¶„í• ê¸°
        self.text_splitter = SmartTextSplitter(max_chunk_size=1000, overlap_sentences=1)

    def get_system_prompt_template(self, system_prompt):
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ - ë²”ìš©ì  ì‚¬ìš©"""
        template = system_prompt + """

ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

**ë‹µë³€ ì§€ì¹¨:**
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ì»¨í…ìŠ¤íŠ¸ì— ê´€ë ¨ ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš°, ì¼ë°˜ì ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë³´ì¶© ì„¤ëª…í•˜ë˜ "(ì¼ë°˜ ì§€ì‹ ê¸°ë°˜)"ì´ë¼ê³  í‘œì‹œí•˜ì„¸ìš”
3. ë‹µë³€í•  ë•ŒëŠ” ì°¸ì¡°í•œ ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”
4. ì •í™•í•œ ì •ë³´ë§Œì„ ì œê³µí•˜ê³ , ì¶”ì¸¡ì´ë‚˜ ê°€ì •ì€ í”¼í•˜ì„¸ìš”
5. ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”

ì»¨í…ìŠ¤íŠ¸:
{context}
"""
        return template

    def create_retriever(self, documents):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ìƒì„± (BM25 + ë²¡í„° ê²€ìƒ‰)"""
        if not documents:
            st.warning("ë¬¸ì„œì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        # ê°œì„ ëœ í…ìŠ¤íŠ¸ ë¶„í•  ì ìš©
        try:
            splits = self.text_splitter.split_documents(documents)
        except Exception as e:
            st.error(f"í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ë¶„í• ê¸°ë¡œ ëŒ€ì²´
            fallback_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=100
            )
            splits = fallback_splitter.split_documents(documents)
        
        if not splits:
            st.warning("ë¬¸ì„œ ë¶„í• ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
        
        # ë¶„í•  ê²°ê³¼ ì •ë³´ í‘œì‹œ
        avg_chunk_length = sum(len(doc.page_content) for doc in splits) / len(splits)
        st.info(f"ğŸ“Š ë¶„í•  ì™„ë£Œ: {len(splits)}ê°œ ì²­í¬, í‰ê·  ê¸¸ì´: {avg_chunk_length:.0f}ì")
        
        try:
            # 1. ë²¡í„° ê²€ìƒ‰ê¸° ìƒì„± (ì˜ë¯¸ì  ìœ ì‚¬ë„)
            vectorstore = FAISS.from_documents(splits, self.embeddings)
            vector_retriever = vectorstore.as_retriever(
                search_type="similarity", 
                search_kwargs={
                    "k": 10,              # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì¦ê°€
                    "score_threshold": 0.15  # ì„ê³„ê°’ ë‚®ì¶¤ (ë” ê´€ëŒ€í•˜ê²Œ)
                }
            )
            
            # 2. BM25 ê²€ìƒ‰ê¸° ìƒì„± (í‚¤ì›Œë“œ ë§¤ì¹­)
            bm25_retriever = BM25Retriever.from_documents(splits)
            bm25_retriever.k = 8  # BM25ë¡œ 8ê°œ ë¬¸ì„œ ê²€ìƒ‰
            
            # 3. í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ê²€ìƒ‰ê¸° ìƒì„±
            ensemble_retriever = EnsembleRetriever(
                retrievers=[bm25_retriever, vector_retriever],
                weights=[0.4, 0.6]  # BM25: 40%, ë²¡í„°: 60%
            )
            
            st.success("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° (BM25 + ë²¡í„°) ìƒì„± ì™„ë£Œ")
            
            # 4. ì••ì¶• ê²€ìƒ‰ê¸° ì ìš© (ì„ íƒì )
            if len(splits) < 30:  # ì‘ì€ ë¬¸ì„œë§Œ ì••ì¶• ê²€ìƒ‰ ì‚¬ìš©
                compressor = LLMChainExtractor.from_llm(self.llm)
                compression_retriever = ContextualCompressionRetriever(
                    base_compressor=compressor,
                    base_retriever=ensemble_retriever
                )
                return compression_retriever
            else:
                return ensemble_retriever
                
        except Exception as e:
            st.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ìƒì„± ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰ê¸°ë§Œ ì‚¬ìš©
            try:
                vectorstore = FAISS.from_documents(splits, self.embeddings)
                return vectorstore.as_retriever(
                    search_type="similarity", 
                    search_kwargs={"k": 10, "score_threshold": 0.15}
                )
            except Exception as e2:
                st.error(f"ê¸°ë³¸ ê²€ìƒ‰ê¸° ìƒì„±ë„ ì‹¤íŒ¨: {e2}")
                return None

    def create_conversational_rag_chain(self, retriever, system_prompt):
        """ëŒ€í™”í˜• RAG ì²´ì¸ ìƒì„±"""
        enhanced_template = self.get_system_prompt_template(system_prompt)
        
        rag_prompt = ChatPromptTemplate.from_messages([
            ("system", enhanced_template),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{input}"),
        ])
        
        document_chain = create_stuff_documents_chain(self.llm, rag_prompt)
        return create_retrieval_chain(retriever, document_chain)

    def create_default_chain(self, system_prompt):
        """ê¸°ë³¸ ì²´ì¸ ìƒì„±"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{question}"),
        ])
        return prompt | self.llm | StrOutputParser()

    def format_chat_history(self, messages):
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…"""
        chat_history = []
        for msg in messages[:-1]:
            if msg["role"] == "user":
                chat_history.append(HumanMessage(content=msg["content"]))
            else:
                chat_history.append(AIMessage(content=msg["content"]))
        return chat_history
