import streamlit as st
from dotenv import load_dotenv
from rag_pipeline import get_retriever_from_source, get_conversational_rag_chain, get_default_chain
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# API KEY ì •ë³´ë¡œë“œ
load_dotenv()

# --- ìƒˆë¡œìš´ ê¸°ëŠ¥: ì¶œì²˜ ì¬í‰ê°€ ë° í•„í„°ë§ í•¨ìˆ˜ ---
def filter_relevant_sources(answer, source_documents):
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ë‹µë³€ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì†ŒìŠ¤ ë¬¸ì„œë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    if not source_documents:
        return []

    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)
    
    context_str = "\n---\n".join(
        [f"[Document {i+1}]: {doc.page_content}" for i, doc in enumerate(source_documents)]
    )

    prompt_template = """
    You are a helpful assistant. Your task is to identify which of the provided source documents are most relevant to the given answer.

    Here is the answer that was generated:
    ---
    {answer}
    ---

    Here are the source documents that were used as context:
    ---
    {context}
    ---

    Please list the numbers of the documents that directly support or contain the information presented in the answer. List the most relevant documents first. If no documents are relevant, respond with "None".

    Example: 3, 1, 8
    Relevant Document Numbers:
    """
    prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = prompt | llm | StrOutputParser()

    try:
        response = chain.invoke({"answer": answer, "context": context_str})
        if response and response.strip().lower() != 'none':
            indices_str = response.strip().split(',')
            relevant_indices = [int(i.strip()) - 1 for i in indices_str if i.strip().isdigit()]
            # ê´€ë ¨ì„± ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ ë¬¸ì„œë¥¼ ë°˜í™˜
            filtered_docs = [source_documents[i] for i in relevant_indices if 0 <= i < len(source_documents)]
            return filtered_docs
        return []
    except Exception as e:
        print(f"Error during source filtering: {e}")
        return []

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="Modular RAG Chatbot", page_icon="ğŸ¤–")
st.title("ğŸ¤– ëª¨ë“ˆí™”ëœ RAG ì±—ë´‡")

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "retriever" not in st.session_state:
    st.session_state.retriever = None
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— í•­ìƒ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."

# --- ì‚¬ì´ë“œë°” UI ---
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    st.divider()
    
    with st.form("persona_form"):
        st.subheader("ğŸ¤– AI í˜ë¥´ì†Œë‚˜ ì„¤ì •")
        system_prompt_input = st.text_area(
            "AIì˜ ì—­í• ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.",
            value=st.session_state.system_prompt,
            height=150
        )
        if st.form_submit_button("í˜ë¥´ì†Œë‚˜ ì ìš©"):
            st.session_state.system_prompt = system_prompt_input
            st.success("í˜ë¥´ì†Œë‚˜ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")

    st.divider()
    
    with st.form("source_form"):
        st.subheader("ğŸ” ë¶„ì„ ëŒ€ìƒ ì„¤ì •")
        url_input = st.text_input("ì›¹ì‚¬ì´íŠ¸ URL", placeholder="https://example.com")
        uploaded_files = st.file_uploader(
            "íŒŒì¼ ì—…ë¡œë“œ (PDF, DOCX, TXT)",
            type=["pdf", "docx", "txt"],
            accept_multiple_files=True
        )

        if st.form_submit_button("ë¶„ì„ ì‹œì‘"):
            source_type = None
            source_input = None
            if uploaded_files:
                source_type = "Files"
                source_input = uploaded_files
            elif url_input:
                source_type = "URL"
                source_input = url_input
            else:
                st.warning("ë¶„ì„í•  URLì„ ì…ë ¥í•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

            if source_type:
                with st.spinner("ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                    st.session_state.retriever = get_retriever_from_source(source_type, source_input)
                
                if st.session_state.retriever:
                    st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
                else:
                    st.error("ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. URLì´ë‚˜ íŒŒì¼ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    st.divider()
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.clear()
        st.rerun()

# --- ë©”ì¸ ì±„íŒ… í™”ë©´ ---
for message in st.session_state["messages"]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "sources" in message and message["sources"]:
            with st.expander("ì°¸ê³ í•œ ì¶œì²˜ ë³´ê¸°"):
                for i, source in enumerate(message["sources"]):
                    st.info(f"**ì¶œì²˜ {i+1}**\n\n{source.page_content}")
                    st.divider()

user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)

    current_system_prompt = st.session_state.system_prompt

    try:
        if st.session_state.retriever:
            chain = get_conversational_rag_chain(st.session_state.retriever, current_system_prompt)
            
            with st.chat_message("assistant"):
                container = st.empty()
                ai_answer = ""
                source_documents = []
                
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    for chunk in chain.stream({"input": user_input}):
                        if "answer" in chunk:
                            ai_answer += chunk["answer"]
                            container.markdown(ai_answer + "â–Œ")
                        if "context" in chunk:
                            source_documents = chunk["context"]
                
                container.markdown(ai_answer)
                
                relevant_sources = []
                if source_documents:
                    with st.spinner("ì¶œì²˜ í™•ì¸ ì¤‘..."):
                        relevant_sources = filter_relevant_sources(ai_answer, source_documents)
                
                if relevant_sources:
                    with st.expander("ì°¸ê³ í•œ ì¶œì²˜ ë³´ê¸°"):
                        # í•„í„°ë§ëœ ì¶œì²˜ë¥¼ ìµœëŒ€ 5ê°œê¹Œì§€ í‘œì‹œ
                        for i, source in enumerate(relevant_sources[:5]):
                            st.info(f"**ì¶œì²˜ {i+1}**\n\n{source.page_content}")
                            st.divider()
                
                st.session_state.messages.append(
                    {"role": "assistant", "content": ai_answer, "sources": relevant_sources[:5]}
                )

        else:
            chain = get_default_chain(current_system_prompt)
            
            with st.chat_message("assistant"):
                container = st.empty()
                ai_answer = ""
                for token in chain.stream({"question": user_input}):
                    ai_answer += token
                    container.markdown(ai_answer + "â–Œ")
                container.markdown(ai_answer)
            
            st.session_state.messages.append(
                {"role": "assistant", "content": ai_answer, "sources": []}
            )
    except Exception as e:
        with st.chat_message("assistant"):
            error_message = f"ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. (ì˜¤ë¥˜: {e})"
            st.error(error_message)
            st.session_state.messages.append({"role": "assistant", "content": error_message, "sources": []})
