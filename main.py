import streamlit as st
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# API KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼
from dotenv import load_dotenv

# API KEY ì •ë³´ë¡œë“œ
load_dotenv()

st.set_page_config(page_title="RAG-enhanced Gemini Chatbot", page_icon="ğŸ”—")
st.title("ğŸ”— RAG ê¸°ëŠ¥ì´ ì¶”ê°€ëœ Gemini ì±—ë´‡")
st.markdown(
    """
ì•ˆë…•í•˜ì„¸ìš”! ì´ ì±—ë´‡ì€ ì¼ë°˜ì ì¸ ëŒ€í™”ë¿ë§Œ ì•„ë‹ˆë¼, ì›¹ì‚¬ì´íŠ¸ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  í•´ë‹¹ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” RAG ê¸°ëŠ¥ë„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤.

**ì‚¬ìš© ë°©ë²•:**
1.  ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•˜ê³  ì‹¶ì€ ì›¹ì‚¬ì´íŠ¸ì˜ ì „ì²´ URLì„ ì…ë ¥í•˜ì„¸ìš”.
2.  'ì‚¬ì´íŠ¸ ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”. (ë¶„ì„ì—ëŠ” ì ì‹œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)
3.  ë¶„ì„ì´ ì™„ë£Œë˜ë©´, í•´ë‹¹ ì›¹ì‚¬ì´íŠ¸ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!
"""
)


# --- RAG íŒŒì´í”„ë¼ì¸ ê´€ë ¨ í•¨ìˆ˜ ---
@st.cache_resource(show_spinner="ì›¹ì‚¬ì´íŠ¸ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")
def get_retriever_from_url(url):
    """
    URLë¡œë¶€í„° ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³ , í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œ ê¸°ë°˜ì˜ retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    Streamlitì˜ cache_resource ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆ ìƒì„±ëœ retrieverëŠ” ì„¸ì…˜ ë‚´ì—ì„œ ì¬ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    # 1. ë¬¸ì„œ ë¡œë“œ
    loader = WebBaseLoader(url)
    documents = loader.load()

    # 2. í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_documents(documents)

    # 3. ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vectorstore = FAISS.from_documents(splits, embeddings)

    # 4. Retriever ìƒì„±
    # # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ 3ê°œë¥¼ ê°€ì ¸ì˜¤ë„ë¡ kê°’ ì„¤ì •
    # return vectorstore.as_retriever(search_kwargs={"k": 3})
    # ìœ ì‚¬ë„ ì ìˆ˜ 0.5 ì´ìƒì¸ ë¬¸ì„œë§Œ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •
    return vectorstore.as_retriever(
        search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.5}
    )


# --- LangChain ì²´ì¸ ìƒì„± í•¨ìˆ˜ ---
def get_conversational_rag_chain(retriever):
    """
    RAG ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ëŠ” ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # RAG í”„ë¡¬í”„íŠ¸ ì •ì˜
    rag_prompt = ChatPromptTemplate.from_template(
        """
        Answer the user's question based on the context provided below.
        If you don't know the answer, just say that you don't know. Don't make up an answer.

        Context:
        {context}

        Question:
        {input}
        """
    )

    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)

    # ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©í•˜ëŠ” ì²´ì¸
    document_chain = create_stuff_documents_chain(llm, rag_prompt)

    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ document_chainìœ¼ë¡œ ì „ë‹¬í•˜ëŠ” ì²´ì¸
    retrieval_chain = create_retrieval_chain(retriever, document_chain)

    return retrieval_chain


def get_default_chain():
    """
    ê¸°ë³¸ì ì¸ ëŒ€í™”í˜• ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
            ("user", "{question}"),
        ]
    )
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)
    output_parser = StrOutputParser()
    return prompt | llm | output_parser


# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "retriever" not in st.session_state:
    st.session_state.retriever = None

# --- ì‚¬ì´ë“œë°” UI ---
with st.sidebar:
    st.header("RAG ì„¤ì •")
    url_input = st.text_input(
        "ë¶„ì„í•  ì›¹ì‚¬ì´íŠ¸ URLì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="https://example.com"
    )

    if st.button("ì‚¬ì´íŠ¸ ë¶„ì„ ì‹œì‘"):
        if url_input:
            st.session_state.retriever = get_retriever_from_url(url_input)
            st.success("ì‚¬ì´íŠ¸ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
        else:
            st.warning("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    st.divider()
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.session_state.retriever = None
        st.rerun()

# --- ë©”ì¸ ì±„íŒ… í™”ë©´ ---
# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
for chat_message in st.session_state["messages"]:
    st.chat_message(chat_message.role).write(chat_message.content)

# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

if user_input:
    st.chat_message("user").write(user_input)

    # RAG ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if st.session_state.retriever:
        chain = get_conversational_rag_chain(st.session_state.retriever)
        # RAG ì²´ì¸ì˜ ì…ë ¥ í˜•ì‹ì€ {"input": user_question} ì…ë‹ˆë‹¤.
        response = chain.stream({"input": user_input})

        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            # RAG ì²´ì¸ì˜ ì‘ë‹µ êµ¬ì¡°ê°€ ë‹¤ë¥´ë¯€ë¡œ, 'answer' í‚¤ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
            for chunk in response:
                if "answer" in chunk:
                    ai_answer += chunk["answer"]
                    container.markdown(ai_answer)
    else:
        # RAG ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ëœ ê²½ìš°, ê¸°ë³¸ ì²´ì¸ ì‚¬ìš©
        chain = get_default_chain()
        response = chain.stream({"question": user_input})

        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            for token in response:
                ai_answer += token
                container.markdown(ai_answer)

    # ëŒ€í™”ê¸°ë¡ ì €ì¥
    st.session_state.messages.append(ChatMessage(role="user", content=user_input))
    st.session_state.messages.append(ChatMessage(role="assistant", content=ai_answer))
