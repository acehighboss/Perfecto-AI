import streamlit as st
from dotenv import load_dotenv
from rag_pipeline import get_retriever_from_source, get_conversational_rag_chain, get_default_chain
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# API KEY ì •ë³´ë¡œë“œ
load_dotenv()

# --- ìƒˆë¡œìš´ ê¸°ëŠ¥: ì¶œì²˜ ì¬í‰ê°€ ë° í•„í„°ë§ í•¨ìˆ˜ ---
def filter_relevant_sources(answer, source_documents):
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ë‹µë³€ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì†ŒìŠ¤ ë¬¸ì„œë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    if not source_documents:
        return []

    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)
    
    # ëª¨ë“  ë¬¸ì„œë¥¼ ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ ë‹¨ì¼ ë¬¸ìì—´ë¡œ ê²°í•©
    context_str = "\n---\n".join(
        [f"[Document {i+1}]: {doc.page_content}" for i, doc in enumerate(source_documents)]
    )

    # LLMì—ê²Œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì‹ë³„í•˜ë„ë¡ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸
    prompt_template = """
    You are a helpful assistant. Your task is to identify which of the provided source documents are relevant to the given answer.

    Here is the answer that was generated:
    ---
    {answer}
    ---

    Here are the source documents that were used as context:
    ---
    {context}
    ---

    Please list the numbers of the documents that directly support or contain the information presented in the answer. The document numbers should be separated by commas. If no documents are relevant, respond with "None".

    Example: 1, 3, 8
    Relevant Document Numbers:
    """
    prompt = ChatPromptTemplate.from_template(prompt_template)

    chain = prompt | llm | StrOutputParser()

    try:
        response = chain.invoke({
            "answer": answer,
            "context": context_str
        })

        if response and response.strip().lower() != 'none':
            indices_str = response.strip().split(',')
            # ì¤‘ë³µì„ ì œê±°í•˜ê³  íš¨ìœ¨ì ì¸ ì¡°íšŒë¥¼ ìœ„í•´ set ì‚¬ìš©
            relevant_indices = {int(i.strip()) - 1 for i in indices_str}
            filtered_docs = [doc for i, doc in enumerate(source_documents) if i in relevant_indices]
            return filtered_docs
        else:
            return []
    except Exception:
        # íŒŒì‹± ì˜¤ë¥˜ë‚˜ ë‹¤ë¥¸ ë¬¸ì œê°€ ë°œìƒí•  ê²½ìš° ì•ˆì „í•˜ê²Œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return []

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="Modular RAG Chatbot", page_icon="ğŸ¤–")
st.title("ğŸ¤– ëª¨ë“ˆí™”ëœ RAG ì±—ë´‡")
st.markdown(
    """
ì•ˆë…•í•˜ì„¸ìš”! ì´ ì±—ë´‡ì€ ì›¹ì‚¬ì´íŠ¸ URLì´ë‚˜ ì—…ë¡œë“œëœ íŒŒì¼(PDF, DOCX, TXT)ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ë‹µë³€í•©ë‹ˆë‹¤.
ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ AIì˜ í˜ë¥´ì†Œë‚˜ì™€ ë¶„ì„í•  ëŒ€ìƒì„ ì„¤ì •í•˜ê³  'ì ìš©' ë˜ëŠ” 'ë¶„ì„' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.
"""
)

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "retriever" not in st.session_state:
    st.session_state.retriever = None
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— í•­ìƒ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."

# --- ì‚¬ì´ë“œë°” UI ---
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    st.divider()
    
    st.subheader("ğŸ¤– AI í˜ë¥´ì†Œë‚˜ ì„¤ì •")
    system_prompt_input = st.text_area(
        "AIì˜ ì—­í• ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.",
        value=st.session_state.system_prompt,
        height=150,
        key="persona_input"
    )
    if st.button("í˜ë¥´ì†Œë‚˜ ì ìš©"):
        st.session_state.system_prompt = system_prompt_input
        st.success("í˜ë¥´ì†Œë‚˜ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")

    st.divider()
    st.subheader("ğŸ” ë¶„ì„ ëŒ€ìƒ ì„¤ì •")
    
    url_input = st.text_input("ì›¹ì‚¬ì´íŠ¸ URL", placeholder="https://example.com")
    
    uploaded_files = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ (PDF, DOCX, TXT)",
        type=["pdf", "docx", "txt"],
        accept_multiple_files=True
    )

    if st.button("ë¶„ì„ ì‹œì‘"):
        source_type = None
        source_input = None
        if uploaded_files:
            source_type = "Files"
            source_input = uploaded_files
        elif url_input:
            source_type = "URL"
            source_input = url_input
        else:
            st.warning("ë¶„ì„í•  URLì„ ì…ë ¥í•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

        if source_type:
            with st.spinner("ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                st.session_state.retriever = get_retriever_from_source(source_type, source_input)
            st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

    st.divider()
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.clear()
        st.rerun()

# --- ë©”ì¸ ì±„íŒ… í™”ë©´ ---
# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
for message in st.session_state["messages"]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "sources" in message and message["sources"]:
            with st.expander("ì°¸ê³ í•œ ì¶œì²˜ ë³´ê¸°"):
                for i, source in enumerate(message["sources"]):
                    st.info(f"**ì¶œì²˜ {i+1}**\n\n{source.page_content}")
                    st.divider()

# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)

    current_system_prompt = st.session_state.system_prompt

    if st.session_state.retriever:
        chain = get_conversational_rag_chain(st.session_state.retriever, current_system_prompt)
        
        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            source_documents = []
            
            # ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë°
            for chunk in chain.stream({"input": user_input}):
                if "answer" in chunk:
                    ai_answer += chunk["answer"]
                    container.markdown(ai_answer + "â–Œ")
                if "context" in chunk:
                    source_documents = chunk["context"]
            
            container.markdown(ai_answer)

            # ë‹µë³€ ìƒì„± í›„, ê´€ë ¨ì„± ë†’ì€ ì¶œì²˜ë§Œ í•„í„°ë§ (ìˆ˜ì •ëœ ë¶€ë¶„)
            relevant_sources = []
            if source_documents:
                with st.spinner("ì¶œì²˜ í™•ì¸ ì¤‘..."):
                    relevant_sources = filter_relevant_sources(ai_answer, source_documents)
            
            # í•„í„°ë§ëœ ì¶œì²˜ í‘œì‹œ
            if relevant_sources:
                with st.expander("ì°¸ê³ í•œ ì¶œì²˜ ë³´ê¸°"):
                    for i, source in enumerate(relevant_sources):
                        st.info(f"**ì¶œì²˜ {i+1}**\n\n{source.page_content}")
                        st.divider()
            
            # í•„í„°ë§ëœ ì¶œì²˜ì™€ í•¨ê»˜ ë©”ì‹œì§€ ì €ì¥
            st.session_state.messages.append(
                {"role": "assistant", "content": ai_answer, "sources": relevant_sources}
            )

    else:
        chain = get_default_chain(current_system_prompt)
        
        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            for token in chain.stream({"question": user_input}):
                ai_answer += token
                container.markdown(ai_answer + "â–Œ")
            container.markdown(ai_answer)
        
        st.session_state.messages.append(
            {"role": "assistant", "content": ai_answer, "sources": []}
        )
