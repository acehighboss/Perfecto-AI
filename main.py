import streamlit as st
import os
import tempfile
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import WebBaseLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.vectorstores import FAISS
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

from llama_parse import LlamaParse

# API KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼
from dotenv import load_dotenv

# API KEY ì •ë³´ë¡œë“œ
load_dotenv()

st.set_page_config(page_title="Multimodal RAG Chatbot", page_icon="ğŸ¤–")
st.title("ğŸ¤– ë©€í‹°ëª¨ë‹¬ íŒŒì¼/URL ë¶„ì„ RAG ì±—ë´‡")
st.markdown(
    """
ì•ˆë…•í•˜ì„¸ìš”! ì´ ì±—ë´‡ì€ ì›¹ì‚¬ì´íŠ¸ URLì´ë‚˜ ì—…ë¡œë“œëœ íŒŒì¼ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ë‹µë³€í•©ë‹ˆë‹¤.
**LlamaParse**ë¥¼ ì‚¬ìš©í•˜ì—¬ **í…Œì´ë¸”ê³¼ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì¸ì‹**í•˜ê³  ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
)


# [ìˆ˜ì • 2: LlamaParseë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë¬¸ì„œ ë¡œë”© í•¨ìˆ˜ ë³€ê²½]
def get_documents_from_files_with_llamaparse(uploaded_files):
    """
    ì—…ë¡œë“œëœ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ì—ì„œ LlamaParseë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    í…Œì´ë¸”ê³¼ í…ìŠ¤íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    all_documents = []

    # LlamaParse íŒŒì„œ ì„¤ì •. ê²°ê³¼ë¬¼ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.
    parser = LlamaParse(
        api_key=os.getenv("LLAMA_CLOUD_API_KEY"),
        result_type="markdown",
        language="ko",
        verbose=True,  # ì§„í–‰ ìƒí™©ì„ ë¡œê·¸ë¡œ í‘œì‹œ
    )

    for uploaded_file in uploaded_files:
        with tempfile.NamedTemporaryFile(
            delete=False, suffix=".pdf" if "pdf" in uploaded_file.type else ".docx"
        ) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

            # LlamaParseë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ íŒŒì‹±
            # LlamaParseëŠ” LangChain Documentê°€ ì•„ë‹Œ ìì²´ Documentë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ,
            # LangChain Documentë¡œ ë³€í™˜í•´ì£¼ëŠ” .load_and_parse()ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            # í•˜ì§€ë§Œ llama-parse ìµœì‹  ë²„ì „ì€ ë°”ë¡œ load_dataë¥¼ ì‚¬ìš©í•´ LangChainê³¼ í˜¸í™˜ë˜ëŠ” ë¬¸ì„œë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # load_dataëŠ” íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ìœ¼ë¯€ë¡œ ë‹¨ì¼ íŒŒì¼ë„ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
            try:
                # llama_parseëŠ” ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë¯€ë¡œ, ì´ë²¤íŠ¸ ë£¨í”„ ê´€ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                # Streamlit í™˜ê²½ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ê°„ë‹¨íˆ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                import asyncio

                documents = asyncio.run(parser.aload_data(tmp_file_path))
                all_documents.extend(documents)
            except Exception as e:
                st.error(f"LlamaParse ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.remove(tmp_file_path)

    return all_documents


@st.cache_resource(show_spinner="LlamaParseë¡œ ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")
def get_retriever_from_source(source_type, source_input, threshold):
    """
    URL ë˜ëŠ” íŒŒì¼ë¡œë¶€í„° ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³ , í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ì—¬ retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    documents = []
    if source_type == "URL":
        loader = WebBaseLoader(source_input)
        documents = loader.load()
    elif source_type == "Files":
        # [ìˆ˜ì • 3: ìƒˆë¡œìš´ LlamaParse í•¨ìˆ˜ í˜¸ì¶œ]
        documents = get_documents_from_files_with_llamaparse(source_input)

    if not documents:
        st.warning("ë¬¸ì„œì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None

    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type="percentile")
    splits = text_splitter.split_documents(documents)
    vectorstore = FAISS.from_documents(splits, embeddings)

    return vectorstore.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={"k": 3, "score_threshold": threshold},
    )


def get_conversational_rag_chain(retriever, system_prompt):
    template = f"""{system_prompt}

Answer the user's question based on the context provided below and the conversation history.
The context may include text and tables in markdown format. You must be able to understand and answer based on them.
If you don't know the answer, just say that you don't know. Don't make up an answer.

Context:
{{context}}
"""
    rag_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", template),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{input}"),
        ]
    )
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)
    document_chain = create_stuff_documents_chain(llm, rag_prompt)
    return create_retrieval_chain(retriever, document_chain)


def get_default_chain(system_prompt):
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{question}"),
        ]
    )
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)
    return prompt | llm | StrOutputParser()


# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "retriever" not in st.session_state:
    st.session_state.retriever = None
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = "ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸”ì„ ì •í™•íˆ ì´í•´í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."


# --- ì‚¬ì´ë“œë°” UI ---
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    st.divider()

    st.subheader("ğŸ¤– AI í˜ë¥´ì†Œë‚˜ ì„¤ì •")
    system_prompt_input = st.text_area(
        "AIì˜ ì—­í• ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.", value=st.session_state.system_prompt, height=150
    )
    st.session_state.system_prompt = system_prompt_input

    st.divider()
    st.subheader("ğŸ” ë¶„ì„ ëŒ€ìƒ ì„¤ì •")

    url_input = st.text_input("ì›¹ì‚¬ì´íŠ¸ URL", placeholder="https://example.com")
    uploaded_files = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ (PDF, DOCX)", type=["pdf", "docx"], accept_multiple_files=True
    )
    st.info(
        "LlamaParseëŠ” í…Œì´ë¸”, í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ë¬¸ì„œ ë¶„ì„ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
        icon="â„¹ï¸",
    )

    st.subheader("ğŸ“Š ê²€ìƒ‰ ì •í™•ë„ ì„¤ì •")
    similarity_threshold = st.slider(
        "ìœ ì‚¬ë„ ì„ê³„ê°’ (ê°’ì´ ë‚®ì„ìˆ˜ë¡ ì •í™•í•¨)",
        min_value=0.0,
        max_value=1.0,
        value=0.7,
        step=0.05,
        help="ë¬¸ì„œ ê²€ìƒ‰ ì‹œ, ì„¤ì •ëœ ê°’ë³´ë‹¤ ë‚®ì€ ê±°ë¦¬(distance)ì˜ ë¬¸ì„œë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤. 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë‚´ìš©ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.",
    )

    if st.button("ë¶„ì„ ì‹œì‘"):
        source_type = None
        source_input = None
        if uploaded_files:
            source_type = "Files"
            source_input = uploaded_files
            st.session_state.retriever = get_retriever_from_source(
                source_type, source_input, similarity_threshold
            )
        elif url_input:
            source_type = "URL"
            source_input = url_input
            st.session_state.retriever = get_retriever_from_source(
                source_type, source_input, similarity_threshold
            )
        else:
            st.warning("ë¶„ì„í•  URLì„ ì…ë ¥í•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

        if st.session_state.retriever:
            st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

    st.divider()
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.clear()
        st.rerun()

# --- ë©”ì¸ ì±„íŒ… í™”ë©´ ---
for message in st.session_state["messages"]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "sources" in message and message["sources"]:
            with st.expander("ì°¸ê³ í•œ ì¶œì²˜ ë³´ê¸° (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)"):
                for i, source in enumerate(message["sources"]):
                    st.text(f"--- ì¶œì²˜ {i+1} ---")
                    st.markdown(source.page_content)


user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)

    chat_history = [
        ChatMessage(role=msg["role"], content=msg["content"])
        for msg in st.session_state.messages[:-1]
    ]

    if st.session_state.retriever:
        chain = get_conversational_rag_chain(
            st.session_state.retriever, st.session_state.system_prompt
        )

        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            source_documents = []

            for chunk in chain.stream(
                {"input": user_input, "chat_history": chat_history}
            ):
                if "answer" in chunk:
                    ai_answer += chunk["answer"]
                    container.markdown(ai_answer)
                if "context" in chunk and not source_documents:
                    source_documents = chunk["context"]

            st.session_state.messages.append(
                {"role": "assistant", "content": ai_answer, "sources": source_documents}
            )

            if source_documents:
                with st.expander("ì°¸ê³ í•œ ì¶œì²˜ ë³´ê¸° (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)"):
                    for i, source in enumerate(source_documents):
                        st.text(f"--- ì¶œì²˜ {i+1} ---")
                        st.markdown(source.page_content)  # ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì¶œë ¥

    else:
        chain = get_default_chain(st.session_state.system_prompt)

        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            for token in chain.stream(
                {"question": user_input, "chat_history": chat_history}
            ):
                ai_answer += token
                container.markdown(ai_answer)

        st.session_state.messages.append(
            {"role": "assistant", "content": ai_answer, "sources": []}
        )
