import streamlit as st
import os
import tempfile
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import (
    WebBaseLoader,
    PyPDFLoader,
    Docx2txtLoader,
)
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.vectorstores import FAISS
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# API KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼
from dotenv import load_dotenv

# API KEY ì •ë³´ë¡œë“œ
load_dotenv()

st.set_page_config(page_title="File/URL RAG Chatbot", page_icon="ğŸ¤–")
st.title("ğŸ¤– íŒŒì¼/URL ë¶„ì„ RAG ì±—ë´‡")
st.markdown(
    """
ì•ˆë…•í•˜ì„¸ìš”! ì´ ì±—ë´‡ì€ ì›¹ì‚¬ì´íŠ¸ URLì´ë‚˜ ì—…ë¡œë“œëœ íŒŒì¼(PDF, DOCX)ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ë‹µë³€í•©ë‹ˆë‹¤.
ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ AIì˜ í˜ë¥´ì†Œë‚˜ì™€ ë¶„ì„í•  ëŒ€ìƒì„ ì„¤ì •í•´ì£¼ì„¸ìš”.
"""
)

# --- ë¬¸ì„œ ë¡œë”© ë° ì²˜ë¦¬ ê´€ë ¨ í•¨ìˆ˜ ---


def get_documents_from_files(uploaded_files):
    """
    ì—…ë¡œë“œëœ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    all_documents = []
    for uploaded_file in uploaded_files:
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ê²½ë¡œë¥¼ ì–»ìŒ
        with tempfile.NamedTemporaryFile(
            delete=False, suffix=os.path.splitext(uploaded_file.name)[1]
        ) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        loader = None
        if uploaded_file.name.endswith(".pdf"):
            loader = PyPDFLoader(tmp_file_path)
        elif uploaded_file.name.endswith(".docx"):
            loader = Docx2txtLoader(tmp_file_path)
        # ì¶”ê°€ì ì¸ íŒŒì¼ í˜•ì‹ ë¡œë”ë¥¼ ì—¬ê¸°ì— ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        if loader:
            all_documents.extend(loader.load())

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(tmp_file_path)

    return all_documents


@st.cache_resource(show_spinner="ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")
def get_retriever_from_source(source_type, source_input, threshold):
    """
    URL ë˜ëŠ” íŒŒì¼ë¡œë¶€í„° ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³ , í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ì—¬ retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    documents = []
    if source_type == "URL":
        loader = WebBaseLoader(source_input)
        documents = loader.load()
    elif source_type == "Files":
        documents = get_documents_from_files(source_input)

    if not documents:
        return None

    # ì„ë² ë”© ëª¨ë¸ ì •ì˜
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

    # SemanticChunkerë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type="percentile")
    splits = text_splitter.split_documents(documents)

    # FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë° retriever ë°˜í™˜
    vectorstore = FAISS.from_documents(splits, embeddings)

    return vectorstore.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={"k": 3, "score_threshold": threshold},
    )


# --- LangChain ì²´ì¸ ìƒì„± í•¨ìˆ˜ ---


# get_conversational_rag_chain: ëŒ€í™” ê¸°ë¡ì„ ì²˜ë¦¬í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
def get_conversational_rag_chain(retriever, system_prompt):
    """
    RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤. ëŒ€í™” ê¸°ë¡(chat_history)ì„ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•©ë‹ˆë‹¤.
    """
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ëŒ€í™” ê¸°ë¡ì„ í™œìš©í•˜ë¼ëŠ” ë‚´ìš©ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    template = f"""{system_prompt}

Answer the user's question based on the context provided below and the conversation history.
If you don't know the answer, just say that you don't know. Don't make up an answer.

Context:
{{context}}
"""
    # MessagesPlaceholderë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ê¸°ë¡ì„ í”„ë¡¬í”„íŠ¸ì— ì‚½ì…í•©ë‹ˆë‹¤.
    rag_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", template),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{input}"),
        ]
    )
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)
    document_chain = create_stuff_documents_chain(llm, rag_prompt)
    return create_retrieval_chain(retriever, document_chain)


# get_default_chain: ëŒ€í™” ê¸°ë¡ì„ ì²˜ë¦¬í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
def get_default_chain(system_prompt):
    """
    ê¸°ë³¸ ëŒ€í™” ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤. ëŒ€í™” ê¸°ë¡(chat_history)ì„ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•©ë‹ˆë‹¤.
    """
    # MessagesPlaceholderë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ê¸°ë¡ì„ í”„ë¡¬í”„íŠ¸ì— ì‚½ì…í•©ë‹ˆë‹¤.
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{question}"),
        ]
    )
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)
    return prompt | llm | StrOutputParser()


# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "retriever" not in st.session_state:
    st.session_state.retriever = None
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— í•­ìƒ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."


# --- ì‚¬ì´ë“œë°” UI ---
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    st.divider()

    st.subheader("ğŸ¤– AI í˜ë¥´ì†Œë‚˜ ì„¤ì •")
    system_prompt_input = st.text_area(
        "AIì˜ ì—­í• ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.", value=st.session_state.system_prompt, height=150
    )
    st.session_state.system_prompt = system_prompt_input

    st.divider()
    st.subheader("ğŸ” ë¶„ì„ ëŒ€ìƒ ì„¤ì •")

    url_input = st.text_input("ì›¹ì‚¬ì´íŠ¸ URL", placeholder="https://example.com")
    uploaded_files = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ (PDF, DOCX)", type=["pdf", "docx"], accept_multiple_files=True
    )
    st.info("ì´ë¯¸ì§€ íŒŒì¼ ë¶„ì„ì€ í˜„ì¬ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", icon="â„¹ï¸")

    st.subheader("ğŸ“Š ê²€ìƒ‰ ì •í™•ë„ ì„¤ì •")
    similarity_threshold = st.slider(
        "ìœ ì‚¬ë„ ì„ê³„ê°’ (ê°’ì´ ë‚®ì„ìˆ˜ë¡ ì •í™•í•¨)",
        min_value=0.0,
        max_value=1.0,
        value=0.7,
        step=0.05,
        help="ë¬¸ì„œ ê²€ìƒ‰ ì‹œ, ì„¤ì •ëœ ê°’ë³´ë‹¤ ë‚®ì€ ê±°ë¦¬(distance)ì˜ ë¬¸ì„œë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤. 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë‚´ìš©ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.",
    )

    if st.button("ë¶„ì„ ì‹œì‘"):
        source_type = None
        source_input = None
        if uploaded_files:
            source_type = "Files"
            source_input = uploaded_files
            st.session_state.retriever = get_retriever_from_source(
                source_type, source_input, similarity_threshold
            )
        elif url_input:
            source_type = "URL"
            source_input = url_input
            st.session_state.retriever = get_retriever_from_source(
                source_type, source_input, similarity_threshold
            )
        else:
            st.warning("ë¶„ì„í•  URLì„ ì…ë ¥í•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

        if st.session_state.retriever:
            st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

    st.divider()
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.clear()
        st.rerun()

# --- ë©”ì¸ ì±„íŒ… í™”ë©´ ---
for message in st.session_state["messages"]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "sources" in message and message["sources"]:
            with st.expander("ì°¸ê³ í•œ ì¶œì²˜ ë³´ê¸°"):
                for i, source in enumerate(message["sources"]):
                    st.info(f"**ì¶œì²˜ {i+1}**\n\n{source.page_content}")
                    st.divider()


user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)

    # ì²´ì¸ì— ì „ë‹¬í•  ëŒ€í™” ê¸°ë¡ ìƒì„±
    # í˜„ì¬ ì…ë ¥ì„ ì œì™¸í•œ ì´ì „ ëŒ€í™” ê¸°ë¡ì„ LangChainì´ ì´í•´í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    chat_history = [
        ChatMessage(role=msg["role"], content=msg["content"])
        for msg in st.session_state.messages[:-1]
    ]

    if st.session_state.retriever:
        chain = get_conversational_rag_chain(
            st.session_state.retriever, st.session_state.system_prompt
        )

        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            source_documents = []

            # RAG ì²´ì¸ í˜¸ì¶œ ì‹œ chat_history ì „ë‹¬
            for chunk in chain.stream(
                {"input": user_input, "chat_history": chat_history}
            ):
                if "answer" in chunk:
                    ai_answer += chunk["answer"]
                    container.markdown(ai_answer)
                if "context" in chunk and not source_documents:
                    source_documents = chunk["context"]

            st.session_state.messages.append(
                {"role": "assistant", "content": ai_answer, "sources": source_documents}
            )

            if source_documents:
                with st.expander("ì°¸ê³ í•œ ì¶œì²˜ ë³´ê¸°"):
                    for i, source in enumerate(source_documents):
                        st.info(f"**ì¶œì²˜ {i+1}**\n\n{source.page_content}")
                        st.divider()

    else:
        chain = get_default_chain(st.session_state.system_prompt)

        with st.chat_message("assistant"):
            container = st.empty()
            ai_answer = ""
            # ê¸°ë³¸ ì²´ì¸ í˜¸ì¶œ ì‹œ chat_history ì „ë‹¬
            for token in chain.stream(
                {"question": user_input, "chat_history": chat_history}
            ):
                ai_answer += token
                container.markdown(ai_answer)

        st.session_state.messages.append(
            {"role": "assistant", "content": ai_answer, "sources": []}
        )
