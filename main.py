import streamlit as st
import os
import tempfile
import asyncio
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.documents import Document as LangChainDocument
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import WebBaseLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.vectorstores import FAISS
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from llama_parse import LlamaParse
from dotenv import load_dotenv

load_dotenv()

st.set_page_config(page_title="Multimodal RAG Chatbot", page_icon="ğŸ¤–")
st.title("ğŸ¤– ë©€í‹°ëª¨ë‹¬ íŒŒì¼/URL ë¶„ì„ RAG ì±—ë´‡")
st.markdown(
    """
ì•ˆë…•í•˜ì„¸ìš”! ì´ ì±—ë´‡ì€ ì›¹ì‚¬ì´íŠ¸ URLì´ë‚˜ ì—…ë¡œë“œëœ íŒŒì¼(PDF, DOCX)ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ë‹µë³€í•©ë‹ˆë‹¤.
**LlamaParse**ë¥¼ ì‚¬ìš©í•˜ì—¬ **í…Œì´ë¸”ê³¼ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì¸ì‹**í•˜ê³  ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
)

def get_documents_from_files_with_llamaparse(uploaded_files):
    async def parse_files(files):
        parser = LlamaParse(
            api_key=os.getenv("LLAMA_CLOUD_API_KEY"),
            result_type="markdown",
            verbose=True,
        )
        parsed_data = []
        for file in files:
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.name)[1]) as tmp_file:
                tmp_file.write(file.getvalue())
                tmp_file_path = tmp_file.name
            
            try:
                documents = await parser.aload_data(tmp_file_path)
                parsed_data.extend(documents)
            except Exception as e:
                st.error(f"LlamaParse ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            finally:
                os.remove(tmp_file_path)
        return parsed_data

    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    return loop.run_until_complete(parse_files(uploaded_files))


@st.cache_resource(show_spinner="LlamaParseë¡œ ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")
def get_retriever_from_source(source_type, source_input): # [ìˆ˜ì • 1] threshold íŒŒë¼ë¯¸í„° ì œê±°
    """
    URL ë˜ëŠ” íŒŒì¼ë¡œë¶€í„° ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³ , í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ì—¬ retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    documents = []
    
    if source_type == "URL":
        loader = WebBaseLoader(source_input)
        documents = loader.load()
    elif source_type == "Files":
        llama_index_documents = get_documents_from_files_with_llamaparse(source_input)

        if llama_index_documents:
            langchain_documents = [
                LangChainDocument(page_content=doc.text, metadata=doc.metadata)
                for doc in llama_index_documents
            ]
            documents.extend(langchain_documents)

    if not documents:
        st.warning("ë¬¸ì„œì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None

    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type="percentile")
    splits = text_splitter.split_documents(documents)
    
    vectorstore = FAISS.from_documents(splits, embeddings)

    # [ìˆ˜ì • 2] ê²€ìƒ‰ ë°©ì‹ì„ 'similarity'ë¡œ ë³€ê²½í•˜ê³ , ìƒìœ„ 5ê°œ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •
    return vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})


def get_conversational_rag_chain(retriever, system_prompt):
    template = f"""{system_prompt}

Answer the user's question based on the context provided below and the conversation history.
The context may include text and tables in markdown format. You must be able to understand and answer based on them.
If you don't know the answer, just say that you don't know. Don't make up an answer.

Context:
{{context}}
"""
    rag_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", template),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{input}"),
        ]
    )
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)
    document_chain = create_stuff_documents_chain(llm, rag_prompt)
    return create_retrieval_chain(retriever, document_chain)


def get_default_chain(system_prompt):
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{question}"),
        ]
    )
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)
    return prompt | llm | StrOutputParser()


# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "retriever" not in st.session_state:
    st.session_state.retriever = None
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = "ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸”ì„ ì •í™•íˆ ì´í•´í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."


# --- ì‚¬ì´ë“œë°” UI ---
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    st.divider()

    st.subheader("ğŸ¤– AI í˜ë¥´ì†Œë‚˜ ì„¤ì •")
    system_prompt_input = st.text_area(
        "AIì˜ ì—­í• ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.", value=st.session_state.system_prompt, height=150
    )
    st.session_state.system_prompt = system_prompt_input

    st.divider()
    st.subheader("ğŸ” ë¶„ì„ ëŒ€ìƒ ì„¤ì •")

    url_input = st.text_input("ì›¹ì‚¬ì´íŠ¸ URL", placeholder="https://example.com")
    uploaded_files = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ (PDF, DOCX)", type=["pdf", "docx"], accept_multiple_files=True
    )
    st.info("LlamaParseëŠ” í…Œì´ë¸”, í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ë¬¸ì„œ ë¶„ì„ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.", icon="â„¹ï¸")
    
    # [ìˆ˜ì • 3] ìœ ì‚¬ë„ ì„ê³„ê°’ ìŠ¬ë¼ì´ë” UI ì œê±°
    # st.subheader("ğŸ“Š ê²€ìƒ‰ ì •í™•ë„ ì„¤ì •")
    # similarity_threshold = st.slider(...)

    if st.button("ë¶„ì„ ì‹œì‘"):
        source_type = None
        source_input = None
        if uploaded_files:
            source_type = "Files"
            source_input = uploaded_files
            # [ìˆ˜ì • 4] get_retriever_from_source í˜¸ì¶œ ì‹œ threshold ì¸ì ì œê±°
            st.session_state.retriever = get_retriever_from_source(
                source_type, source_input
            )
        elif url_input:
            source_type = "URL"
            source_input = url_input
            st.session_state.retriever = get_retriever_from_source(
                source_type, source_input
            )
        else:
            st.warning("ë¶„ì„í•  URLì„ ì…ë ¥í•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

        if st.session_state.retriever:
            st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

    st.divider()
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.clear()
        st.rerun()

# --- ë©”ì¸ ì±„íŒ… í™”ë©´ ---
for message in st.session_state["messages"]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "sources" in message and message["sources"]:
            with st.expander("ì°¸ê³ í•œ ì¶œì²˜ ë³´ê¸° (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)"):
                for i, source in enumerate(message["sources"]):
                    st.text(f"--- ì¶œì²˜ {i+1} ---")
                    st.markdown(source.page_content)


user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)

    try:
        chat_history = [
            HumanMessage(content=msg["content"]) if msg["role"] == "user" 
            else AIMessage(content=msg["content"])
            for msg in st.session_state.messages[:-1]
        ]

        if st.session_state.retriever:
            chain = get_conversational_rag_chain(
                st.session_state.retriever, st.session_state.system_prompt
            )

            with st.chat_message("assistant"):
                container = st.empty()
                ai_answer = ""
                source_documents = []

                for chunk in chain.stream(
                    {"input": user_input, "chat_history": chat_history}
                ):
                    if "answer" in chunk:
                        ai_answer += chunk["answer"]
                        container.markdown(ai_answer)
                    if "context" in chunk and not source_documents:
                        source_documents = chunk["context"]
                
                st.session_state.messages.append(
                    {"role": "assistant", "content": ai_answer, "sources": source_documents}
                )

                if source_documents:
                    with st.expander("ì°¸ê³ í•œ ì¶œì²˜ ë³´ê¸° (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)"):
                        for i, source in enumerate(source_documents):
                            st.text(f"--- ì¶œì²˜ {i+1} ---")
                            st.markdown(source.page_content)

        else:
            chain = get_default_chain(st.session_state.system_prompt)

            with st.chat_message("assistant"):
                container = st.empty()
                ai_answer = ""
                for token in chain.stream(
                    {"question": user_input, "chat_history": chat_history}
                ):
                    ai_answer += token
                    container.markdown(ai_answer)
                
                st.session_state.messages.append(
                    {"role": "assistant", "content": ai_answer, "sources": []}
                )

    except Exception as e:
        st.chat_message("assistant").error(f"ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\nì˜¤ë¥˜: {e}")
        st.session_state.messages.pop()
